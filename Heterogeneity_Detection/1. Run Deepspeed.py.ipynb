{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085d7c6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-30 00:18:15,343] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-30 00:18:15,622] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2024-10-30 00:18:15,645] [INFO] [runner.py:571:main] cmd = /opt/conda/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29508 --enable_each_rank_log=None 1_train_heterogeneity_deepspeed.py\n",
      "[2024-10-30 00:18:17,767] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-30 00:18:17,975] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.16.2-1\n",
      "[2024-10-30 00:18:17,975] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.16.2-1\n",
      "[2024-10-30 00:18:17,975] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
      "[2024-10-30 00:18:17,975] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
      "[2024-10-30 00:18:17,975] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.16.2-1+cuda11.8\n",
      "[2024-10-30 00:18:17,975] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.16.2-1+cuda11.8\n",
      "[2024-10-30 00:18:17,975] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.2-1\n",
      "[2024-10-30 00:18:17,975] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}\n",
      "[2024-10-30 00:18:17,975] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0\n",
      "[2024-10-30 00:18:17,975] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})\n",
      "[2024-10-30 00:18:17,975] [INFO] [launch.py:163:main] dist_world_size=4\n",
      "[2024-10-30 00:18:17,975] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3\n",
      "[2024-10-30 00:18:22,915] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-30 00:18:22,929] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-30 00:18:22,949] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-30 00:18:22,949] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "train_valid_test: 733_92_92\n",
      "train_valid_test: 733_92_92\n",
      "train_valid_test: 733_92_92\n",
      "train_valid_test: 733_92_92\n",
      "model_param_num = 24001133\n",
      "[2024-10-30 00:18:24,522] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.12.6, git-hash=unknown, git-branch=unknown\n",
      "[2024-10-30 00:18:24,522] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "model_param_num = 24001133\n",
      "[2024-10-30 00:18:24,526] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.12.6, git-hash=unknown, git-branch=unknown\n",
      "[2024-10-30 00:18:24,526] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "model_param_num = 24001133\n",
      "[2024-10-30 00:18:24,530] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.12.6, git-hash=unknown, git-branch=unknown\n",
      "[2024-10-30 00:18:24,531] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "model_param_num = 24001133\n",
      "[2024-10-30 00:18:24,539] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.12.6, git-hash=unknown, git-branch=unknown\n",
      "[2024-10-30 00:18:24,539] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-10-30 00:18:24,539] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-10-30 00:18:26,422] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
      "Creating extension directory /root/.cache/torch_extensions/py310_cu118/cpu_adam...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
      "[1/4] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_75,code=compute_75 -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o \n",
      "[2/4] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o \n",
      "[3/4] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o \n",
      "[4/4] c++ cpu_adam.o cpu_adam_impl.o custom_cuda_kernel.cuda.o -shared -lcurand -L/opt/conda/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 33.26639413833618 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 33.18783640861511 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 33.181684255599976 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 33.178738832473755 seconds\n",
      "Adam Optimizer #0 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "Adam Optimizer #0 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "[2024-10-30 00:19:02,166] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
      "[2024-10-30 00:19:02,167] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "Adam Optimizer #0 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "Adam Optimizer #0 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "[2024-10-30 00:19:02,198] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "[2024-10-30 00:19:02,198] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "[2024-10-30 00:19:02,198] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer\n",
      "[2024-10-30 00:19:02,198] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 200000000\n",
      "[2024-10-30 00:19:02,198] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 200000000\n",
      "[2024-10-30 00:19:02,198] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: True\n",
      "[2024-10-30 00:19:02,198] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-30 00:19:02,787] [INFO] [utils.py:791:see_memory_usage] Before initializing optimizer states\n",
      "[2024-10-30 00:19:02,789] [INFO] [utils.py:792:see_memory_usage] MA 0.11 GB         Max_MA 0.11 GB         CA 0.12 GB         Max_CA 0 GB \n",
      "[2024-10-30 00:19:02,789] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 12.26 GB, percent = 4.9%\n",
      "[2024-10-30 00:19:03,066] [INFO] [utils.py:791:see_memory_usage] After initializing optimizer states\n",
      "[2024-10-30 00:19:03,067] [INFO] [utils.py:792:see_memory_usage] MA 0.11 GB         Max_MA 0.11 GB         CA 0.12 GB         Max_CA 0 GB \n",
      "[2024-10-30 00:19:03,068] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 12.38 GB, percent = 4.9%\n",
      "[2024-10-30 00:19:03,068] [INFO] [stage_1_and_2.py:516:__init__] optimizer state initialized\n",
      "[2024-10-30 00:19:03,210] [INFO] [utils.py:791:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-10-30 00:19:03,211] [INFO] [utils.py:792:see_memory_usage] MA 0.11 GB         Max_MA 0.11 GB         CA 0.12 GB         Max_CA 0 GB \n",
      "[2024-10-30 00:19:03,211] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 12.39 GB, percent = 4.9%\n",
      "[2024-10-30 00:19:03,219] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n",
      "[2024-10-30 00:19:03,219] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = OneCycle\n",
      "[2024-10-30 00:19:03,219] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.OneCycle object at 0x7fc0360a3a30>\n",
      "[2024-10-30 00:19:03,220] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[4e-05], mom=[(0.8, 0.99)]\n",
      "[2024-10-30 00:19:03,223] [INFO] [config.py:984:print] DeepSpeedEngine configuration:\n",
      "[2024-10-30 00:19:03,223] [INFO] [config.py:988:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-10-30 00:19:03,223] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-10-30 00:19:03,223] [INFO] [config.py:988:print]   amp_enabled .................. False\n",
      "[2024-10-30 00:19:03,223] [INFO] [config.py:988:print]   amp_params ................... False\n",
      "[2024-10-30 00:19:03,223] [INFO] [config.py:988:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-10-30 00:19:03,223] [INFO] [config.py:988:print]   bfloat16_enabled ............. False\n",
      "[2024-10-30 00:19:03,223] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-10-30 00:19:03,223] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-10-30 00:19:03,223] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-10-30 00:19:03,223] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc0360a3220>\n",
      "[2024-10-30 00:19:03,223] [INFO] [config.py:988:print]   communication_data_type ...... None\n",
      "[2024-10-30 00:19:03,223] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-10-30 00:19:03,223] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False\n",
      "[2024-10-30 00:19:03,223] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False\n",
      "[2024-10-30 00:19:03,223] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   dataloader_drop_last ......... False\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   disable_allgather ............ False\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   dump_state ................... False\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   elasticity_enabled ........... False\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   fp16_auto_cast ............... None\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   fp16_enabled ................. False\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   global_rank .................. 0\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   grad_accum_dtype ............. None\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   gradient_clipping ............ 1.0\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   graph_harvesting ............. False\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 65536\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   load_universal_checkpoint .... False\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   loss_scale ................... 0\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   memory_breakdown ............. False\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   mics_shard_size .............. -1\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   optimizer_name ............... adam\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   optimizer_params ............. {'lr': 1e-05, 'betas': [0.9, 0.999]}\n",
      "[2024-10-30 00:19:03,224] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-10-30 00:19:03,225] [INFO] [config.py:988:print]   pld_enabled .................. False\n",
      "[2024-10-30 00:19:03,225] [INFO] [config.py:988:print]   pld_params ................... False\n",
      "[2024-10-30 00:19:03,225] [INFO] [config.py:988:print]   prescale_gradients ........... False\n",
      "[2024-10-30 00:19:03,225] [INFO] [config.py:988:print]   scheduler_name ............... OneCycle\n",
      "[2024-10-30 00:19:03,225] [INFO] [config.py:988:print]   scheduler_params ............. {'cycle_min_lr': 4e-05, 'cycle_max_lr': 0.0002, 'decay_lr_rate': 0.25, 'cycle_first_step_size': 184, 'cycle_second_step_size': 460, 'decay_step_size': 276}\n",
      "[2024-10-30 00:19:03,225] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-10-30 00:19:03,225] [INFO] [config.py:988:print]   sparse_attention ............. None\n",
      "[2024-10-30 00:19:03,225] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False\n",
      "[2024-10-30 00:19:03,225] [INFO] [config.py:988:print]   steps_per_print .............. 92\n",
      "[2024-10-30 00:19:03,225] [INFO] [config.py:988:print]   train_batch_size ............. 8\n",
      "[2024-10-30 00:19:03,225] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  2\n",
      "[2024-10-30 00:19:03,225] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False\n",
      "[2024-10-30 00:19:03,225] [INFO] [config.py:988:print]   use_node_local_storage ....... False\n",
      "[2024-10-30 00:19:03,225] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False\n",
      "[2024-10-30 00:19:03,225] [INFO] [config.py:988:print]   weight_quantization_config ... None\n",
      "[2024-10-30 00:19:03,225] [INFO] [config.py:988:print]   world_size ................... 4\n",
      "[2024-10-30 00:19:03,225] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False\n",
      "[2024-10-30 00:19:03,225] [INFO] [config.py:988:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=PosixPath('/local_nvme'), buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=PosixPath('/local_nvme'), buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-10-30 00:19:03,225] [INFO] [config.py:988:print]   zero_enabled ................. True\n",
      "[2024-10-30 00:19:03,225] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-10-30 00:19:03,225] [INFO] [config.py:988:print]   zero_optimization_stage ...... 2\n",
      "[2024-10-30 00:19:03,225] [INFO] [config.py:974:print_user_config]   json = {\n",
      "    \"train_batch_size\": 8, \n",
      "    \"train_micro_batch_size_per_gpu\": 2, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"pipeline_parallel\": true, \n",
      "    \"steps_per_print\": 92, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"hysteresis\": 2, \n",
      "        \"consecutive_hysteresis\": false, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"nvme_path\": \"/local_nvme\", \n",
      "            \"pin_memory\": true, \n",
      "            \"buffer_count\": 4, \n",
      "            \"fast_init\": false\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"nvme_path\": \"/local_nvme\", \n",
      "            \"pin_memory\": true, \n",
      "            \"buffer_count\": 5, \n",
      "            \"buffer_size\": 1.000000e+08, \n",
      "            \"max_in_cpu\": 1.000000e+09\n",
      "        }\n",
      "    }, \n",
      "    \"data_sampling\": {\n",
      "        \"enabled\": true, \n",
      "        \"num_workers\": 16\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"OneCycle\", \n",
      "        \"params\": {\n",
      "            \"cycle_min_lr\": 4e-05, \n",
      "            \"cycle_max_lr\": 0.0002, \n",
      "            \"decay_lr_rate\": 0.25, \n",
      "            \"cycle_first_step_size\": 184, \n",
      "            \"cycle_second_step_size\": 460, \n",
      "            \"decay_step_size\": 276\n",
      "        }\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"Adam\", \n",
      "        \"params\": {\n",
      "            \"lr\": 1e-05, \n",
      "            \"betas\": [0.9, 0.999]\n",
      "        }\n",
      "    }\n",
      "}\n",
      "[2024-10-30 00:19:03,225] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.6, git-hash=unknown, git-branch=unknown\n",
      "[2024-10-30 00:19:03,268] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam Optimizer #1 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "Adam Optimizer #1 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "Adam Optimizer #1 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "Adam Optimizer #1 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "[2024-10-30 00:19:05,014] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
      "[2024-10-30 00:19:05,014] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-10-30 00:19:05,045] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "[2024-10-30 00:19:05,045] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "[2024-10-30 00:19:05,045] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer\n",
      "[2024-10-30 00:19:05,045] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 200000000\n",
      "[2024-10-30 00:19:05,045] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 200000000\n",
      "[2024-10-30 00:19:05,045] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: True\n",
      "[2024-10-30 00:19:05,045] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False\n",
      "[2024-10-30 00:19:05,402] [INFO] [utils.py:791:see_memory_usage] Before initializing optimizer states\n",
      "[2024-10-30 00:19:05,403] [INFO] [utils.py:792:see_memory_usage] MA 0.21 GB         Max_MA 0.21 GB         CA 0.23 GB         Max_CA 0 GB \n",
      "[2024-10-30 00:19:05,403] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 17.01 GB, percent = 6.8%\n",
      "[2024-10-30 00:19:05,616] [INFO] [utils.py:791:see_memory_usage] After initializing optimizer states\n",
      "[2024-10-30 00:19:05,617] [INFO] [utils.py:792:see_memory_usage] MA 0.21 GB         Max_MA 0.21 GB         CA 0.23 GB         Max_CA 0 GB \n",
      "[2024-10-30 00:19:05,617] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 17.1 GB, percent = 6.8%\n",
      "[2024-10-30 00:19:05,617] [INFO] [stage_1_and_2.py:516:__init__] optimizer state initialized\n",
      "[2024-10-30 00:19:05,765] [INFO] [utils.py:791:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-10-30 00:19:05,766] [INFO] [utils.py:792:see_memory_usage] MA 0.21 GB         Max_MA 0.21 GB         CA 0.23 GB         Max_CA 0 GB \n",
      "[2024-10-30 00:19:05,766] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 17.11 GB, percent = 6.8%\n",
      "[2024-10-30 00:19:05,775] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n",
      "[2024-10-30 00:19:05,775] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = OneCycle\n",
      "[2024-10-30 00:19:05,775] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.OneCycle object at 0x7fbffc162c20>\n",
      "[2024-10-30 00:19:05,776] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[4e-05], mom=[(0.8, 0.99)]\n",
      "[2024-10-30 00:19:05,779] [INFO] [config.py:984:print] DeepSpeedEngine configuration:\n",
      "[2024-10-30 00:19:05,779] [INFO] [config.py:988:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-10-30 00:19:05,779] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-10-30 00:19:05,779] [INFO] [config.py:988:print]   amp_enabled .................. False\n",
      "[2024-10-30 00:19:05,779] [INFO] [config.py:988:print]   amp_params ................... False\n",
      "[2024-10-30 00:19:05,779] [INFO] [config.py:988:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-10-30 00:19:05,779] [INFO] [config.py:988:print]   bfloat16_enabled ............. False\n",
      "[2024-10-30 00:19:05,779] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-10-30 00:19:05,779] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-10-30 00:19:05,779] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-10-30 00:19:05,779] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc0360a33a0>\n",
      "[2024-10-30 00:19:05,779] [INFO] [config.py:988:print]   communication_data_type ...... None\n",
      "[2024-10-30 00:19:05,779] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-10-30 00:19:05,779] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False\n",
      "[2024-10-30 00:19:05,779] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False\n",
      "[2024-10-30 00:19:05,779] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-10-30 00:19:05,779] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False\n",
      "[2024-10-30 00:19:05,779] [INFO] [config.py:988:print]   dataloader_drop_last ......... False\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   disable_allgather ............ False\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   dump_state ................... False\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   elasticity_enabled ........... False\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   fp16_auto_cast ............... None\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   fp16_enabled ................. False\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   global_rank .................. 0\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   grad_accum_dtype ............. None\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   gradient_clipping ............ 1.0\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   graph_harvesting ............. False\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 65536\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   load_universal_checkpoint .... False\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   loss_scale ................... 0\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   memory_breakdown ............. False\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   mics_shard_size .............. -1\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   optimizer_name ............... adam\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   optimizer_params ............. {'lr': 1e-05, 'betas': [0.9, 0.999]}\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   pld_enabled .................. False\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   pld_params ................... False\n",
      "[2024-10-30 00:19:05,780] [INFO] [config.py:988:print]   prescale_gradients ........... False\n",
      "[2024-10-30 00:19:05,781] [INFO] [config.py:988:print]   scheduler_name ............... OneCycle\n",
      "[2024-10-30 00:19:05,781] [INFO] [config.py:988:print]   scheduler_params ............. {'cycle_min_lr': 4e-05, 'cycle_max_lr': 0.0002, 'decay_lr_rate': 0.25, 'cycle_first_step_size': 184, 'cycle_second_step_size': 460, 'decay_step_size': 276}\n",
      "[2024-10-30 00:19:05,781] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-10-30 00:19:05,781] [INFO] [config.py:988:print]   sparse_attention ............. None\n",
      "[2024-10-30 00:19:05,781] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False\n",
      "[2024-10-30 00:19:05,781] [INFO] [config.py:988:print]   steps_per_print .............. 92\n",
      "[2024-10-30 00:19:05,781] [INFO] [config.py:988:print]   train_batch_size ............. 8\n",
      "[2024-10-30 00:19:05,781] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  2\n",
      "[2024-10-30 00:19:05,781] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False\n",
      "[2024-10-30 00:19:05,781] [INFO] [config.py:988:print]   use_node_local_storage ....... False\n",
      "[2024-10-30 00:19:05,781] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False\n",
      "[2024-10-30 00:19:05,781] [INFO] [config.py:988:print]   weight_quantization_config ... None\n",
      "[2024-10-30 00:19:05,781] [INFO] [config.py:988:print]   world_size ................... 4\n",
      "[2024-10-30 00:19:05,781] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False\n",
      "[2024-10-30 00:19:05,781] [INFO] [config.py:988:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=PosixPath('/local_nvme'), buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=PosixPath('/local_nvme'), buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-10-30 00:19:05,781] [INFO] [config.py:988:print]   zero_enabled ................. True\n",
      "[2024-10-30 00:19:05,781] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-10-30 00:19:05,781] [INFO] [config.py:988:print]   zero_optimization_stage ...... 2\n",
      "[2024-10-30 00:19:05,781] [INFO] [config.py:974:print_user_config]   json = {\n",
      "    \"train_batch_size\": 8, \n",
      "    \"train_micro_batch_size_per_gpu\": 2, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"pipeline_parallel\": true, \n",
      "    \"steps_per_print\": 92, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"hysteresis\": 2, \n",
      "        \"consecutive_hysteresis\": false, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"nvme_path\": \"/local_nvme\", \n",
      "            \"pin_memory\": true, \n",
      "            \"buffer_count\": 4, \n",
      "            \"fast_init\": false\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"nvme_path\": \"/local_nvme\", \n",
      "            \"pin_memory\": true, \n",
      "            \"buffer_count\": 5, \n",
      "            \"buffer_size\": 1.000000e+08, \n",
      "            \"max_in_cpu\": 1.000000e+09\n",
      "        }\n",
      "    }, \n",
      "    \"data_sampling\": {\n",
      "        \"enabled\": true, \n",
      "        \"num_workers\": 16\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"OneCycle\", \n",
      "        \"params\": {\n",
      "            \"cycle_min_lr\": 4e-05, \n",
      "            \"cycle_max_lr\": 0.0002, \n",
      "            \"decay_lr_rate\": 0.25, \n",
      "            \"cycle_first_step_size\": 184, \n",
      "            \"cycle_second_step_size\": 460, \n",
      "            \"decay_step_size\": 276\n",
      "        }\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"Adam\", \n",
      "        \"params\": {\n",
      "            \"lr\": 1e-05, \n",
      "            \"betas\": [0.9, 0.999]\n",
      "        }\n",
      "    }\n",
      "}\n",
      "[2024-10-30 00:19:05,781] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.6, git-hash=unknown, git-branch=unknown\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-30 00:19:05,824] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "Adam Optimizer #2 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "Adam Optimizer #2 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "Adam Optimizer #2 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "Adam Optimizer #2 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "[2024-10-30 00:19:07,401] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
      "[2024-10-30 00:19:07,401] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-10-30 00:19:07,429] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "[2024-10-30 00:19:07,429] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "[2024-10-30 00:19:07,429] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer\n",
      "[2024-10-30 00:19:07,429] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 200000000\n",
      "[2024-10-30 00:19:07,429] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 200000000\n",
      "[2024-10-30 00:19:07,429] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: True\n",
      "[2024-10-30 00:19:07,429] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False\n",
      "[2024-10-30 00:19:07,886] [INFO] [utils.py:791:see_memory_usage] Before initializing optimizer states\n",
      "[2024-10-30 00:19:07,887] [INFO] [utils.py:792:see_memory_usage] MA 0.21 GB         Max_MA 0.32 GB         CA 0.33 GB         Max_CA 0 GB \n",
      "[2024-10-30 00:19:07,888] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 20.7 GB, percent = 8.2%\n",
      "[2024-10-30 00:19:08,094] [INFO] [utils.py:791:see_memory_usage] After initializing optimizer states\n",
      "[2024-10-30 00:19:08,095] [INFO] [utils.py:792:see_memory_usage] MA 0.21 GB         Max_MA 0.21 GB         CA 0.33 GB         Max_CA 0 GB \n",
      "[2024-10-30 00:19:08,095] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 20.92 GB, percent = 8.3%\n",
      "[2024-10-30 00:19:08,095] [INFO] [stage_1_and_2.py:516:__init__] optimizer state initialized\n",
      "[2024-10-30 00:19:08,261] [INFO] [utils.py:791:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-10-30 00:19:08,262] [INFO] [utils.py:792:see_memory_usage] MA 0.21 GB         Max_MA 0.21 GB         CA 0.33 GB         Max_CA 0 GB \n",
      "[2024-10-30 00:19:08,262] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 21.23 GB, percent = 8.4%\n",
      "[2024-10-30 00:19:08,271] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n",
      "[2024-10-30 00:19:08,271] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = OneCycle\n",
      "[2024-10-30 00:19:08,271] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.OneCycle object at 0x7fbffc1d0460>\n",
      "[2024-10-30 00:19:08,271] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[4e-05], mom=[(0.8, 0.99)]\n",
      "[2024-10-30 00:19:08,275] [INFO] [config.py:984:print] DeepSpeedEngine configuration:\n",
      "[2024-10-30 00:19:08,275] [INFO] [config.py:988:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-10-30 00:19:08,275] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-10-30 00:19:08,275] [INFO] [config.py:988:print]   amp_enabled .................. False\n",
      "[2024-10-30 00:19:08,275] [INFO] [config.py:988:print]   amp_params ................... False\n",
      "[2024-10-30 00:19:08,275] [INFO] [config.py:988:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-10-30 00:19:08,275] [INFO] [config.py:988:print]   bfloat16_enabled ............. False\n",
      "[2024-10-30 00:19:08,275] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-10-30 00:19:08,275] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-10-30 00:19:08,275] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-10-30 00:19:08,275] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc0360a30a0>\n",
      "[2024-10-30 00:19:08,275] [INFO] [config.py:988:print]   communication_data_type ...... None\n",
      "[2024-10-30 00:19:08,275] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-10-30 00:19:08,275] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False\n",
      "[2024-10-30 00:19:08,275] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False\n",
      "[2024-10-30 00:19:08,275] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-10-30 00:19:08,275] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False\n",
      "[2024-10-30 00:19:08,275] [INFO] [config.py:988:print]   dataloader_drop_last ......... False\n",
      "[2024-10-30 00:19:08,275] [INFO] [config.py:988:print]   disable_allgather ............ False\n",
      "[2024-10-30 00:19:08,275] [INFO] [config.py:988:print]   dump_state ................... False\n",
      "[2024-10-30 00:19:08,275] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-10-30 00:19:08,275] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False\n",
      "[2024-10-30 00:19:08,275] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-10-30 00:19:08,275] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   elasticity_enabled ........... False\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   fp16_auto_cast ............... None\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   fp16_enabled ................. False\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   global_rank .................. 0\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   grad_accum_dtype ............. None\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   gradient_clipping ............ 1.0\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   graph_harvesting ............. False\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 65536\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   load_universal_checkpoint .... False\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   loss_scale ................... 0\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   memory_breakdown ............. False\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   mics_shard_size .............. -1\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   optimizer_name ............... adam\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   optimizer_params ............. {'lr': 1e-05, 'betas': [0.9, 0.999]}\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   pld_enabled .................. False\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   pld_params ................... False\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   prescale_gradients ........... False\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   scheduler_name ............... OneCycle\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   scheduler_params ............. {'cycle_min_lr': 4e-05, 'cycle_max_lr': 0.0002, 'decay_lr_rate': 0.25, 'cycle_first_step_size': 184, 'cycle_second_step_size': 460, 'decay_step_size': 276}\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   sparse_attention ............. None\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False\n",
      "[2024-10-30 00:19:08,276] [INFO] [config.py:988:print]   steps_per_print .............. 92\n",
      "[2024-10-30 00:19:08,277] [INFO] [config.py:988:print]   train_batch_size ............. 8\n",
      "[2024-10-30 00:19:08,277] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  2\n",
      "[2024-10-30 00:19:08,277] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False\n",
      "[2024-10-30 00:19:08,277] [INFO] [config.py:988:print]   use_node_local_storage ....... False\n",
      "[2024-10-30 00:19:08,277] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False\n",
      "[2024-10-30 00:19:08,277] [INFO] [config.py:988:print]   weight_quantization_config ... None\n",
      "[2024-10-30 00:19:08,277] [INFO] [config.py:988:print]   world_size ................... 4\n",
      "[2024-10-30 00:19:08,277] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False\n",
      "[2024-10-30 00:19:08,277] [INFO] [config.py:988:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=PosixPath('/local_nvme'), buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=PosixPath('/local_nvme'), buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-10-30 00:19:08,277] [INFO] [config.py:988:print]   zero_enabled ................. True\n",
      "[2024-10-30 00:19:08,277] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-10-30 00:19:08,277] [INFO] [config.py:988:print]   zero_optimization_stage ...... 2\n",
      "[2024-10-30 00:19:08,277] [INFO] [config.py:974:print_user_config]   json = {\n",
      "    \"train_batch_size\": 8, \n",
      "    \"train_micro_batch_size_per_gpu\": 2, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"pipeline_parallel\": true, \n",
      "    \"steps_per_print\": 92, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"hysteresis\": 2, \n",
      "        \"consecutive_hysteresis\": false, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"nvme_path\": \"/local_nvme\", \n",
      "            \"pin_memory\": true, \n",
      "            \"buffer_count\": 4, \n",
      "            \"fast_init\": false\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"nvme_path\": \"/local_nvme\", \n",
      "            \"pin_memory\": true, \n",
      "            \"buffer_count\": 5, \n",
      "            \"buffer_size\": 1.000000e+08, \n",
      "            \"max_in_cpu\": 1.000000e+09\n",
      "        }\n",
      "    }, \n",
      "    \"data_sampling\": {\n",
      "        \"enabled\": true, \n",
      "        \"num_workers\": 16\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"OneCycle\", \n",
      "        \"params\": {\n",
      "            \"cycle_min_lr\": 4e-05, \n",
      "            \"cycle_max_lr\": 0.0002, \n",
      "            \"decay_lr_rate\": 0.25, \n",
      "            \"cycle_first_step_size\": 184, \n",
      "            \"cycle_second_step_size\": 460, \n",
      "            \"decay_step_size\": 276\n",
      "        }\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"Adam\", \n",
      "        \"params\": {\n",
      "            \"lr\": 1e-05, \n",
      "            \"betas\": [0.9, 0.999]\n",
      "        }\n",
      "    }\n",
      "}\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./result/3d_unet_custom_propotional_bce_8_32_class_recon_fold_0/log.csv check exist...\n",
      "./result/3d_unet_custom_propotional_bce_8_32_class_recon_fold_0/log.csv exist.\n",
      "./result/3d_unet_custom_propotional_bce_8_32_class_recon_fold_0/log.csv has been deleted.\n",
      "  0%|                                                    | 0/92 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1268: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392020201/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])\n",
      "/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1268: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392020201/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])\n",
      "/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1268: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392020201/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])\n",
      "/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1268: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392020201/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])\n",
      " 99%|▉| 91/92 [15:27<00:08,  8.41s/it, Epoch=1/10, loss=0.8960, dice_score=0.362[2024-10-30 00:34:43,221] [INFO] [logging.py:96:log_dist] [Rank 0] step=92, skipped=0, lr=[0.00011999999999999996], mom=[(0.8500000000000001, 0.99)]\n",
      "[2024-10-30 00:34:43,260] [INFO] [timer.py:260:stop] epoch=0/micro_step=92/global_step=92, RunningAvgSamplesPerSec=0.8103627434622612, CurrSamplesPerSec=1.0572113685338953, MemAllocated=0.55GB, MaxMemAllocated=20.55GB\n",
      "100%|█| 92/92 [15:35<00:00, 10.16s/it, Epoch=1/10, loss=0.8923, dice_score=0.367\n",
      "100%|███████████████████████████████████████████| 12/12 [12:14<00:00, 61.18s/it]\n",
      "1 - 0.8923 - 0.3670 - 0.7120 - 0.1762 - 1.0000\n",
      " 99%|▉| 91/92 [14:46<00:07,  7.49s/it, Epoch=2/10, loss=0.5822, dice_score=0.728[2024-10-30 01:01:53,203] [INFO] [logging.py:96:log_dist] [Rank 0] step=184, skipped=0, lr=[0.00019999999999999993], mom=[(0.8, 0.99)]\n",
      "[2024-10-30 01:01:53,241] [INFO] [timer.py:260:stop] epoch=0/micro_step=184/global_step=184, RunningAvgSamplesPerSec=0.8171688146145801, CurrSamplesPerSec=1.1790087423809816, MemAllocated=0.55GB, MaxMemAllocated=20.55GB\n",
      "100%|█| 92/92 [14:53<00:00,  9.72s/it, Epoch=2/10, loss=0.5827, dice_score=0.726\n",
      "100%|███████████████████████████████████████████| 12/12 [12:58<00:00, 64.84s/it]\n",
      "2 - 0.5827 - 0.7269 - 0.7269 - 0.0004 - 1.0000\n",
      " 99%|▉| 91/92 [15:24<00:07,  7.61s/it, Epoch=3/10, loss=0.5830, dice_score=0.743[2024-10-30 01:30:23,755] [INFO] [logging.py:96:log_dist] [Rank 0] step=276, skipped=0, lr=[0.000168], mom=[(0.8200000000000001, 0.99)]\n",
      "[2024-10-30 01:30:23,761] [INFO] [timer.py:260:stop] epoch=0/micro_step=276/global_step=276, RunningAvgSamplesPerSec=0.808318406838246, CurrSamplesPerSec=1.3483344221721607, MemAllocated=0.55GB, MaxMemAllocated=20.55GB\n",
      "100%|█| 92/92 [15:30<00:00, 10.12s/it, Epoch=3/10, loss=0.5849, dice_score=0.741\n",
      "100%|███████████████████████████████████████████| 12/12 [12:40<00:00, 63.41s/it]\n",
      "3 - 0.5849 - 0.7418 - 0.7418 - 0.0001 - 1.0000\n",
      " 99%|▉| 91/92 [16:00<00:07,  7.79s/it, Epoch=4/10, loss=0.5738, dice_score=0.732[2024-10-30 01:59:13,513] [INFO] [logging.py:96:log_dist] [Rank 0] step=368, skipped=0, lr=[0.000136], mom=[(0.8400000000000001, 0.99)]\n",
      "[2024-10-30 01:59:13,549] [INFO] [timer.py:260:stop] epoch=0/micro_step=368/global_step=368, RunningAvgSamplesPerSec=0.7960340594001867, CurrSamplesPerSec=1.2046600795961522, MemAllocated=0.55GB, MaxMemAllocated=20.55GB\n",
      "100%|█| 92/92 [16:07<00:00, 10.51s/it, Epoch=4/10, loss=0.5730, dice_score=0.732\n",
      "100%|███████████████████████████████████████████| 12/12 [12:35<00:00, 62.98s/it]\n",
      "4 - 0.5730 - 0.7323 - 0.7323 - 0.0000 - 1.0000\n",
      " 99%|▉| 91/92 [15:38<00:08,  8.60s/it, Epoch=5/10, loss=0.5669, dice_score=0.719[2024-10-30 02:27:34,135] [INFO] [logging.py:96:log_dist] [Rank 0] step=460, skipped=0, lr=[0.00010399999999999998], mom=[(0.8600000000000001, 0.99)]\n",
      "[2024-10-30 02:27:34,185] [INFO] [timer.py:260:stop] epoch=0/micro_step=460/global_step=460, RunningAvgSamplesPerSec=0.7929613431851625, CurrSamplesPerSec=1.7968144625797884, MemAllocated=0.55GB, MaxMemAllocated=20.55GB\n",
      "100%|█| 92/92 [15:43<00:00, 10.25s/it, Epoch=5/10, loss=0.5678, dice_score=0.720\n",
      "100%|███████████████████████████████████████████| 12/12 [11:09<00:00, 55.83s/it]\n",
      "5 - 0.5678 - 0.7201 - 0.7201 - 0.0000 - 1.0000\n",
      " 99%|▉| 91/92 [14:44<00:07,  7.68s/it, Epoch=6/10, loss=0.5174, dice_score=0.734[2024-10-30 02:53:38,886] [INFO] [logging.py:96:log_dist] [Rank 0] step=552, skipped=0, lr=[7.199999999999999e-05], mom=[(0.88, 0.99)]\n",
      "[2024-10-30 02:53:38,953] [INFO] [timer.py:260:stop] epoch=0/micro_step=552/global_step=552, RunningAvgSamplesPerSec=0.7980658561785451, CurrSamplesPerSec=0.9718783942460812, MemAllocated=0.55GB, MaxMemAllocated=20.55GB\n",
      "100%|█| 92/92 [14:53<00:00,  9.71s/it, Epoch=6/10, loss=0.5166, dice_score=0.735\n",
      "100%|███████████████████████████████████████████| 12/12 [13:05<00:00, 65.43s/it]\n",
      "6 - 0.5166 - 0.7351 - 0.7351 - 0.0000 - 1.0000\n",
      " 99%|▉| 91/92 [15:20<00:07,  7.46s/it, Epoch=7/10, loss=0.4926, dice_score=0.729[2024-10-30 03:22:16,660] [INFO] [logging.py:96:log_dist] [Rank 0] step=644, skipped=0, lr=[4e-05], mom=[(0.9, 0.99)]\n",
      "[2024-10-30 03:22:16,700] [INFO] [timer.py:260:stop] epoch=0/micro_step=644/global_step=644, RunningAvgSamplesPerSec=0.7970535460591776, CurrSamplesPerSec=0.8059561491904249, MemAllocated=0.55GB, MaxMemAllocated=20.55GB\n",
      "100%|█| 92/92 [15:31<00:00, 10.12s/it, Epoch=7/10, loss=0.4890, dice_score=0.732\n",
      "100%|███████████████████████████████████████████| 12/12 [11:34<00:00, 57.89s/it]\n",
      "7 - 0.4890 - 0.7323 - 0.7432 - 0.0000 - 1.0000\n",
      " 99%|▉| 91/92 [14:46<00:07,  7.51s/it, Epoch=8/10, loss=0.5041, dice_score=0.717[2024-10-30 03:48:47,140] [INFO] [logging.py:96:log_dist] [Rank 0] step=736, skipped=0, lr=[3.692307692307693e-05], mom=[(0.9, 0.99)]\n",
      "[2024-10-30 03:48:47,181] [INFO] [timer.py:260:stop] epoch=0/micro_step=736/global_step=736, RunningAvgSamplesPerSec=0.8003073461876947, CurrSamplesPerSec=1.0655242960482114, MemAllocated=0.55GB, MaxMemAllocated=20.55GB\n",
      "100%|█| 92/92 [14:54<00:00,  9.72s/it, Epoch=8/10, loss=0.5032, dice_score=0.717\n",
      "100%|███████████████████████████████████████████| 12/12 [11:11<00:00, 55.97s/it]\n",
      "8 - 0.5032 - 0.7174 - 0.7446 - 0.0000 - 1.0000\n",
      " 99%|▉| 91/92 [14:40<00:07,  7.37s/it, Epoch=9/10, loss=0.5008, dice_score=0.725[2024-10-30 04:14:49,892] [INFO] [logging.py:96:log_dist] [Rank 0] step=828, skipped=0, lr=[3.4285714285714284e-05], mom=[(0.9, 0.99)]\n",
      "[2024-10-30 04:14:49,924] [INFO] [timer.py:260:stop] epoch=0/micro_step=828/global_step=828, RunningAvgSamplesPerSec=0.8033341291754201, CurrSamplesPerSec=0.9482969465734502, MemAllocated=0.55GB, MaxMemAllocated=20.55GB\n",
      "100%|█| 92/92 [14:49<00:00,  9.67s/it, Epoch=9/10, loss=0.4998, dice_score=0.725\n",
      "100%|███████████████████████████████████████████| 12/12 [12:15<00:00, 61.33s/it]\n",
      "9 - 0.4998 - 0.7255 - 0.7527 - 0.0000 - 1.0000\n",
      " 99%|▉| 91/92 [15:05<00:08,  8.15s/it, Epoch=10/10, loss=0.4834, dice_score=0.74[2024-10-30 04:42:17,240] [INFO] [logging.py:96:log_dist] [Rank 0] step=920, skipped=0, lr=[3.2000000000000005e-05], mom=[(0.9, 0.99)]\n",
      "[2024-10-30 04:42:17,278] [INFO] [timer.py:260:stop] epoch=0/micro_step=920/global_step=920, RunningAvgSamplesPerSec=0.8039706930861523, CurrSamplesPerSec=2.1008917185647777, MemAllocated=0.55GB, MaxMemAllocated=20.55GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█| 92/92 [15:09<00:00,  9.89s/it, Epoch=10/10, loss=0.4836, dice_score=0.74\n",
      "100%|███████████████████████████████████████████| 12/12 [11:15<00:00, 56.30s/it]\n",
      "10 - 0.4836 - 0.7405 - 0.7459 - 0.0000 - 1.0000\n",
      "train_valid_test: 733_92_92\n",
      "train_valid_test: 733_92_92\n",
      "model_param_num = 24001133\n",
      "train_valid_test: 733_92_92\n",
      "model_param_num = 24001133\n",
      "train_valid_test: 733_92_92\n",
      "model_param_num = 24001133\n",
      "model_param_num = 24001133\n",
      "[2024-10-30 04:53:39,881] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.6, git-hash=unknown, git-branch=unknown\n",
      "Adam Optimizer #3 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "Adam Optimizer #3 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "[2024-10-30 04:53:40,948] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "Adam Optimizer #3 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "Adam Optimizer #3 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "[2024-10-30 04:53:42,975] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
      "[2024-10-30 04:53:42,975] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-10-30 04:53:43,007] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "[2024-10-30 04:53:43,015] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "Adam Optimizer #4 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "[2024-10-30 04:53:43,052] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer\n",
      "[2024-10-30 04:53:43,076] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 200000000\n",
      "[2024-10-30 04:53:43,076] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 200000000\n",
      "[2024-10-30 04:53:43,076] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: True\n",
      "[2024-10-30 04:53:43,076] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False\n",
      "Adam Optimizer #4 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "Adam Optimizer #4 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "[2024-10-30 04:53:55,036] [INFO] [utils.py:791:see_memory_usage] Before initializing optimizer states\n",
      "[2024-10-30 04:53:55,037] [INFO] [utils.py:792:see_memory_usage] MA 0.48 GB         Max_MA 0.59 GB         CA 0.72 GB         Max_CA 1 GB \n",
      "[2024-10-30 04:53:55,037] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 65.28 GB, percent = 26.0%\n",
      "[2024-10-30 04:53:55,225] [INFO] [utils.py:791:see_memory_usage] After initializing optimizer states\n",
      "[2024-10-30 04:53:55,226] [INFO] [utils.py:792:see_memory_usage] MA 0.48 GB         Max_MA 0.48 GB         CA 0.72 GB         Max_CA 1 GB \n",
      "[2024-10-30 04:53:55,226] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 65.34 GB, percent = 26.0%\n",
      "[2024-10-30 04:53:55,226] [INFO] [stage_1_and_2.py:516:__init__] optimizer state initialized\n",
      "[2024-10-30 04:53:55,385] [INFO] [utils.py:791:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-10-30 04:53:55,386] [INFO] [utils.py:792:see_memory_usage] MA 0.48 GB         Max_MA 0.48 GB         CA 0.72 GB         Max_CA 1 GB \n",
      "[2024-10-30 04:53:55,386] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 65.34 GB, percent = 26.0%\n",
      "[2024-10-30 04:53:55,396] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n",
      "[2024-10-30 04:53:55,396] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = OneCycle\n",
      "[2024-10-30 04:53:55,396] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.OneCycle object at 0x7fc0801b8d00>\n",
      "[2024-10-30 04:53:55,396] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[4e-05], mom=[(0.8, 0.99)]\n",
      "[2024-10-30 04:53:55,399] [INFO] [config.py:984:print] DeepSpeedEngine configuration:\n",
      "[2024-10-30 04:53:55,399] [INFO] [config.py:988:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-10-30 04:53:55,399] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-10-30 04:53:55,399] [INFO] [config.py:988:print]   amp_enabled .................. False\n",
      "[2024-10-30 04:53:55,399] [INFO] [config.py:988:print]   amp_params ................... False\n",
      "[2024-10-30 04:53:55,400] [INFO] [config.py:988:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-10-30 04:53:55,400] [INFO] [config.py:988:print]   bfloat16_enabled ............. False\n",
      "[2024-10-30 04:53:55,400] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-10-30 04:53:55,400] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-10-30 04:53:55,400] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc0801b8670>\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   communication_data_type ...... None\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   dataloader_drop_last ......... False\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   disable_allgather ............ False\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   dump_state ................... False\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   elasticity_enabled ........... False\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   fp16_auto_cast ............... None\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   fp16_enabled ................. False\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   global_rank .................. 0\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   grad_accum_dtype ............. None\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   gradient_clipping ............ 1.0\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   graph_harvesting ............. False\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 65536\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   load_universal_checkpoint .... False\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   loss_scale ................... 0\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   memory_breakdown ............. False\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   mics_shard_size .............. -1\n",
      "[2024-10-30 04:53:55,401] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-10-30 04:53:55,402] [INFO] [config.py:988:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-10-30 04:53:55,402] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-10-30 04:53:55,402] [INFO] [config.py:988:print]   optimizer_name ............... adam\n",
      "[2024-10-30 04:53:55,402] [INFO] [config.py:988:print]   optimizer_params ............. {'lr': 1e-05, 'betas': [0.9, 0.999]}\n",
      "[2024-10-30 04:53:55,402] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-10-30 04:53:55,402] [INFO] [config.py:988:print]   pld_enabled .................. False\n",
      "[2024-10-30 04:53:55,402] [INFO] [config.py:988:print]   pld_params ................... False\n",
      "[2024-10-30 04:53:55,402] [INFO] [config.py:988:print]   prescale_gradients ........... False\n",
      "[2024-10-30 04:53:55,402] [INFO] [config.py:988:print]   scheduler_name ............... OneCycle\n",
      "[2024-10-30 04:53:55,402] [INFO] [config.py:988:print]   scheduler_params ............. {'cycle_min_lr': 4e-05, 'cycle_max_lr': 0.0002, 'decay_lr_rate': 0.25, 'cycle_first_step_size': 184, 'cycle_second_step_size': 460, 'decay_step_size': 276}\n",
      "[2024-10-30 04:53:55,402] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-10-30 04:53:55,402] [INFO] [config.py:988:print]   sparse_attention ............. None\n",
      "[2024-10-30 04:53:55,402] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False\n",
      "[2024-10-30 04:53:55,402] [INFO] [config.py:988:print]   steps_per_print .............. 92\n",
      "[2024-10-30 04:53:55,402] [INFO] [config.py:988:print]   train_batch_size ............. 8\n",
      "[2024-10-30 04:53:55,402] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  2\n",
      "[2024-10-30 04:53:55,402] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False\n",
      "[2024-10-30 04:53:55,402] [INFO] [config.py:988:print]   use_node_local_storage ....... False\n",
      "[2024-10-30 04:53:55,402] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False\n",
      "[2024-10-30 04:53:55,402] [INFO] [config.py:988:print]   weight_quantization_config ... None\n",
      "[2024-10-30 04:53:55,402] [INFO] [config.py:988:print]   world_size ................... 4\n",
      "[2024-10-30 04:53:55,402] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False\n",
      "[2024-10-30 04:53:55,402] [INFO] [config.py:988:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=PosixPath('/local_nvme'), buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=PosixPath('/local_nvme'), buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-10-30 04:53:55,402] [INFO] [config.py:988:print]   zero_enabled ................. True\n",
      "[2024-10-30 04:53:55,402] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-10-30 04:53:55,402] [INFO] [config.py:988:print]   zero_optimization_stage ...... 2\n",
      "[2024-10-30 04:53:55,402] [INFO] [config.py:974:print_user_config]   json = {\n",
      "    \"train_batch_size\": 8, \n",
      "    \"train_micro_batch_size_per_gpu\": 2, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"pipeline_parallel\": true, \n",
      "    \"steps_per_print\": 92, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"hysteresis\": 2, \n",
      "        \"consecutive_hysteresis\": false, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"nvme_path\": \"/local_nvme\", \n",
      "            \"pin_memory\": true, \n",
      "            \"buffer_count\": 4, \n",
      "            \"fast_init\": false\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"nvme_path\": \"/local_nvme\", \n",
      "            \"pin_memory\": true, \n",
      "            \"buffer_count\": 5, \n",
      "            \"buffer_size\": 1.000000e+08, \n",
      "            \"max_in_cpu\": 1.000000e+09\n",
      "        }\n",
      "    }, \n",
      "    \"data_sampling\": {\n",
      "        \"enabled\": true, \n",
      "        \"num_workers\": 16\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"OneCycle\", \n",
      "        \"params\": {\n",
      "            \"cycle_min_lr\": 4e-05, \n",
      "            \"cycle_max_lr\": 0.0002, \n",
      "            \"decay_lr_rate\": 0.25, \n",
      "            \"cycle_first_step_size\": 184, \n",
      "            \"cycle_second_step_size\": 460, \n",
      "            \"decay_step_size\": 276\n",
      "        }\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"Adam\", \n",
      "        \"params\": {\n",
      "            \"lr\": 1e-05, \n",
      "            \"betas\": [0.9, 0.999]\n",
      "        }\n",
      "    }\n",
      "}\n",
      "[2024-10-30 04:53:55,403] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.6, git-hash=unknown, git-branch=unknown\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-30 04:53:55,446] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "Adam Optimizer #4 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "[2024-10-30 04:53:57,180] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
      "[2024-10-30 04:53:57,181] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-10-30 04:53:57,209] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "[2024-10-30 04:53:57,209] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "[2024-10-30 04:53:57,210] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer\n",
      "[2024-10-30 04:53:57,210] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 200000000\n",
      "[2024-10-30 04:53:57,210] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 200000000\n",
      "[2024-10-30 04:53:57,210] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: True\n",
      "[2024-10-30 04:53:57,210] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False\n",
      "Adam Optimizer #5 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "Adam Optimizer #5 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "Adam Optimizer #5 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "[2024-10-30 04:53:58,519] [INFO] [utils.py:791:see_memory_usage] Before initializing optimizer states\n",
      "[2024-10-30 04:53:58,520] [INFO] [utils.py:792:see_memory_usage] MA 0.59 GB         Max_MA 0.59 GB         CA 0.7 GB         Max_CA 1 GB \n",
      "[2024-10-30 04:53:58,520] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 69.8 GB, percent = 27.7%\n",
      "[2024-10-30 04:53:58,695] [INFO] [utils.py:791:see_memory_usage] After initializing optimizer states\n",
      "[2024-10-30 04:53:58,696] [INFO] [utils.py:792:see_memory_usage] MA 0.59 GB         Max_MA 0.59 GB         CA 0.7 GB         Max_CA 1 GB \n",
      "[2024-10-30 04:53:58,696] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 69.86 GB, percent = 27.8%\n",
      "[2024-10-30 04:53:58,696] [INFO] [stage_1_and_2.py:516:__init__] optimizer state initialized\n",
      "[2024-10-30 04:53:58,853] [INFO] [utils.py:791:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-10-30 04:53:58,854] [INFO] [utils.py:792:see_memory_usage] MA 0.59 GB         Max_MA 0.59 GB         CA 0.7 GB         Max_CA 1 GB \n",
      "[2024-10-30 04:53:58,854] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 69.86 GB, percent = 27.8%\n",
      "[2024-10-30 04:53:58,864] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n",
      "[2024-10-30 04:53:58,864] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = OneCycle\n",
      "[2024-10-30 04:53:58,864] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.OneCycle object at 0x7fbffc144130>\n",
      "[2024-10-30 04:53:58,864] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[4e-05], mom=[(0.8, 0.99)]\n",
      "[2024-10-30 04:53:58,867] [INFO] [config.py:984:print] DeepSpeedEngine configuration:\n",
      "[2024-10-30 04:53:58,867] [INFO] [config.py:988:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-10-30 04:53:58,867] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-10-30 04:53:58,867] [INFO] [config.py:988:print]   amp_enabled .................. False\n",
      "[2024-10-30 04:53:58,867] [INFO] [config.py:988:print]   amp_params ................... False\n",
      "[2024-10-30 04:53:58,868] [INFO] [config.py:988:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-10-30 04:53:58,868] [INFO] [config.py:988:print]   bfloat16_enabled ............. False\n",
      "[2024-10-30 04:53:58,868] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-10-30 04:53:58,868] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-10-30 04:53:58,868] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-10-30 04:53:58,868] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc0801b87f0>\n",
      "[2024-10-30 04:53:58,868] [INFO] [config.py:988:print]   communication_data_type ...... None\n",
      "[2024-10-30 04:53:58,868] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-10-30 04:53:58,868] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False\n",
      "[2024-10-30 04:53:58,868] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False\n",
      "[2024-10-30 04:53:58,868] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-10-30 04:53:58,868] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False\n",
      "[2024-10-30 04:53:58,868] [INFO] [config.py:988:print]   dataloader_drop_last ......... False\n",
      "[2024-10-30 04:53:58,868] [INFO] [config.py:988:print]   disable_allgather ............ False\n",
      "[2024-10-30 04:53:58,868] [INFO] [config.py:988:print]   dump_state ................... False\n",
      "[2024-10-30 04:53:58,868] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-10-30 04:53:58,868] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False\n",
      "[2024-10-30 04:53:58,868] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-10-30 04:53:58,868] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-10-30 04:53:58,868] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-10-30 04:53:58,868] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-10-30 04:53:58,868] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-10-30 04:53:58,868] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-10-30 04:53:58,868] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False\n",
      "[2024-10-30 04:53:58,868] [INFO] [config.py:988:print]   elasticity_enabled ........... False\n",
      "[2024-10-30 04:53:58,868] [INFO] [config.py:988:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-10-30 04:53:58,868] [INFO] [config.py:988:print]   fp16_auto_cast ............... None\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   fp16_enabled ................. False\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   global_rank .................. 0\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   grad_accum_dtype ............. None\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   gradient_clipping ............ 1.0\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   graph_harvesting ............. False\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 65536\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   load_universal_checkpoint .... False\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   loss_scale ................... 0\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   memory_breakdown ............. False\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   mics_shard_size .............. -1\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   optimizer_name ............... adam\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   optimizer_params ............. {'lr': 1e-05, 'betas': [0.9, 0.999]}\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   pld_enabled .................. False\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   pld_params ................... False\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   prescale_gradients ........... False\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   scheduler_name ............... OneCycle\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   scheduler_params ............. {'cycle_min_lr': 4e-05, 'cycle_max_lr': 0.0002, 'decay_lr_rate': 0.25, 'cycle_first_step_size': 184, 'cycle_second_step_size': 460, 'decay_step_size': 276}\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   sparse_attention ............. None\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   steps_per_print .............. 92\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   train_batch_size ............. 8\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  2\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   use_node_local_storage ....... False\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   weight_quantization_config ... None\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   world_size ................... 4\n",
      "[2024-10-30 04:53:58,869] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False\n",
      "[2024-10-30 04:53:58,870] [INFO] [config.py:988:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=PosixPath('/local_nvme'), buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=PosixPath('/local_nvme'), buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-10-30 04:53:58,870] [INFO] [config.py:988:print]   zero_enabled ................. True\n",
      "[2024-10-30 04:53:58,870] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-10-30 04:53:58,870] [INFO] [config.py:988:print]   zero_optimization_stage ...... 2\n",
      "[2024-10-30 04:53:58,870] [INFO] [config.py:974:print_user_config]   json = {\n",
      "    \"train_batch_size\": 8, \n",
      "    \"train_micro_batch_size_per_gpu\": 2, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"pipeline_parallel\": true, \n",
      "    \"steps_per_print\": 92, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"hysteresis\": 2, \n",
      "        \"consecutive_hysteresis\": false, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"nvme_path\": \"/local_nvme\", \n",
      "            \"pin_memory\": true, \n",
      "            \"buffer_count\": 4, \n",
      "            \"fast_init\": false\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"nvme_path\": \"/local_nvme\", \n",
      "            \"pin_memory\": true, \n",
      "            \"buffer_count\": 5, \n",
      "            \"buffer_size\": 1.000000e+08, \n",
      "            \"max_in_cpu\": 1.000000e+09\n",
      "        }\n",
      "    }, \n",
      "    \"data_sampling\": {\n",
      "        \"enabled\": true, \n",
      "        \"num_workers\": 16\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"OneCycle\", \n",
      "        \"params\": {\n",
      "            \"cycle_min_lr\": 4e-05, \n",
      "            \"cycle_max_lr\": 0.0002, \n",
      "            \"decay_lr_rate\": 0.25, \n",
      "            \"cycle_first_step_size\": 184, \n",
      "            \"cycle_second_step_size\": 460, \n",
      "            \"decay_step_size\": 276\n",
      "        }\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"Adam\", \n",
      "        \"params\": {\n",
      "            \"lr\": 1e-05, \n",
      "            \"betas\": [0.9, 0.999]\n",
      "        }\n",
      "    }\n",
      "}\n",
      "[2024-10-30 04:53:58,870] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.6, git-hash=unknown, git-branch=unknown\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-30 04:53:58,913] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "Adam Optimizer #5 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "[2024-10-30 04:54:00,606] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
      "[2024-10-30 04:54:00,606] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-10-30 04:54:00,634] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "[2024-10-30 04:54:00,634] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "[2024-10-30 04:54:00,634] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer\n",
      "[2024-10-30 04:54:00,634] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 200000000\n",
      "[2024-10-30 04:54:00,634] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 200000000\n",
      "[2024-10-30 04:54:00,634] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: True\n",
      "[2024-10-30 04:54:00,634] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False\n",
      "[2024-10-30 04:54:01,103] [INFO] [utils.py:791:see_memory_usage] Before initializing optimizer states\n",
      "[2024-10-30 04:54:01,103] [INFO] [utils.py:792:see_memory_usage] MA 0.59 GB         Max_MA 0.69 GB         CA 0.79 GB         Max_CA 1 GB \n",
      "[2024-10-30 04:54:01,104] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 71.24 GB, percent = 28.3%\n",
      "[2024-10-30 04:54:01,309] [INFO] [utils.py:791:see_memory_usage] After initializing optimizer states\n",
      "[2024-10-30 04:54:01,310] [INFO] [utils.py:792:see_memory_usage] MA 0.59 GB         Max_MA 0.59 GB         CA 0.79 GB         Max_CA 1 GB \n",
      "[2024-10-30 04:54:01,311] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 71.09 GB, percent = 28.3%\n",
      "[2024-10-30 04:54:01,311] [INFO] [stage_1_and_2.py:516:__init__] optimizer state initialized\n",
      "[2024-10-30 04:54:01,464] [INFO] [utils.py:791:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-10-30 04:54:01,465] [INFO] [utils.py:792:see_memory_usage] MA 0.59 GB         Max_MA 0.59 GB         CA 0.79 GB         Max_CA 1 GB \n",
      "[2024-10-30 04:54:01,465] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 71.12 GB, percent = 28.3%\n",
      "[2024-10-30 04:54:01,475] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n",
      "[2024-10-30 04:54:01,475] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = OneCycle\n",
      "[2024-10-30 04:54:01,475] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.OneCycle object at 0x7fc080247fa0>\n",
      "[2024-10-30 04:54:01,476] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[4e-05], mom=[(0.8, 0.99)]\n",
      "[2024-10-30 04:54:01,478] [INFO] [config.py:984:print] DeepSpeedEngine configuration:\n",
      "[2024-10-30 04:54:01,479] [INFO] [config.py:988:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-10-30 04:54:01,479] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-10-30 04:54:01,479] [INFO] [config.py:988:print]   amp_enabled .................. False\n",
      "[2024-10-30 04:54:01,479] [INFO] [config.py:988:print]   amp_params ................... False\n",
      "[2024-10-30 04:54:01,479] [INFO] [config.py:988:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-10-30 04:54:01,479] [INFO] [config.py:988:print]   bfloat16_enabled ............. False\n",
      "[2024-10-30 04:54:01,479] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-10-30 04:54:01,479] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-10-30 04:54:01,479] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-10-30 04:54:01,479] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc03658dcf0>\n",
      "[2024-10-30 04:54:01,479] [INFO] [config.py:988:print]   communication_data_type ...... None\n",
      "[2024-10-30 04:54:01,479] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-10-30 04:54:01,479] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False\n",
      "[2024-10-30 04:54:01,479] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False\n",
      "[2024-10-30 04:54:01,479] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-10-30 04:54:01,479] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False\n",
      "[2024-10-30 04:54:01,479] [INFO] [config.py:988:print]   dataloader_drop_last ......... False\n",
      "[2024-10-30 04:54:01,479] [INFO] [config.py:988:print]   disable_allgather ............ False\n",
      "[2024-10-30 04:54:01,479] [INFO] [config.py:988:print]   dump_state ................... False\n",
      "[2024-10-30 04:54:01,479] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-10-30 04:54:01,479] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False\n",
      "[2024-10-30 04:54:01,479] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-10-30 04:54:01,479] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-10-30 04:54:01,479] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   elasticity_enabled ........... False\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   fp16_auto_cast ............... None\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   fp16_enabled ................. False\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   global_rank .................. 0\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   grad_accum_dtype ............. None\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   gradient_clipping ............ 1.0\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   graph_harvesting ............. False\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 65536\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   load_universal_checkpoint .... False\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   loss_scale ................... 0\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   memory_breakdown ............. False\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   mics_shard_size .............. -1\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   optimizer_name ............... adam\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   optimizer_params ............. {'lr': 1e-05, 'betas': [0.9, 0.999]}\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   pld_enabled .................. False\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   pld_params ................... False\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   prescale_gradients ........... False\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   scheduler_name ............... OneCycle\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   scheduler_params ............. {'cycle_min_lr': 4e-05, 'cycle_max_lr': 0.0002, 'decay_lr_rate': 0.25, 'cycle_first_step_size': 184, 'cycle_second_step_size': 460, 'decay_step_size': 276}\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   sparse_attention ............. None\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   steps_per_print .............. 92\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   train_batch_size ............. 8\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  2\n",
      "[2024-10-30 04:54:01,480] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False\n",
      "[2024-10-30 04:54:01,481] [INFO] [config.py:988:print]   use_node_local_storage ....... False\n",
      "[2024-10-30 04:54:01,481] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False\n",
      "[2024-10-30 04:54:01,481] [INFO] [config.py:988:print]   weight_quantization_config ... None\n",
      "[2024-10-30 04:54:01,481] [INFO] [config.py:988:print]   world_size ................... 4\n",
      "[2024-10-30 04:54:01,481] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False\n",
      "[2024-10-30 04:54:01,481] [INFO] [config.py:988:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=PosixPath('/local_nvme'), buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=PosixPath('/local_nvme'), buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-10-30 04:54:01,481] [INFO] [config.py:988:print]   zero_enabled ................. True\n",
      "[2024-10-30 04:54:01,481] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-10-30 04:54:01,481] [INFO] [config.py:988:print]   zero_optimization_stage ...... 2\n",
      "[2024-10-30 04:54:01,481] [INFO] [config.py:974:print_user_config]   json = {\n",
      "    \"train_batch_size\": 8, \n",
      "    \"train_micro_batch_size_per_gpu\": 2, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"pipeline_parallel\": true, \n",
      "    \"steps_per_print\": 92, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"hysteresis\": 2, \n",
      "        \"consecutive_hysteresis\": false, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"nvme_path\": \"/local_nvme\", \n",
      "            \"pin_memory\": true, \n",
      "            \"buffer_count\": 4, \n",
      "            \"fast_init\": false\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"nvme_path\": \"/local_nvme\", \n",
      "            \"pin_memory\": true, \n",
      "            \"buffer_count\": 5, \n",
      "            \"buffer_size\": 1.000000e+08, \n",
      "            \"max_in_cpu\": 1.000000e+09\n",
      "        }\n",
      "    }, \n",
      "    \"data_sampling\": {\n",
      "        \"enabled\": true, \n",
      "        \"num_workers\": 16\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"OneCycle\", \n",
      "        \"params\": {\n",
      "            \"cycle_min_lr\": 4e-05, \n",
      "            \"cycle_max_lr\": 0.0002, \n",
      "            \"decay_lr_rate\": 0.25, \n",
      "            \"cycle_first_step_size\": 184, \n",
      "            \"cycle_second_step_size\": 460, \n",
      "            \"decay_step_size\": 276\n",
      "        }\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"Adam\", \n",
      "        \"params\": {\n",
      "            \"lr\": 1e-05, \n",
      "            \"betas\": [0.9, 0.999]\n",
      "        }\n",
      "    }\n",
      "}\n",
      "torch.float32\n",
      "./result/3d_unet_custom_propotional_bce_8_32_class_recon_fold_1/log.csv check exist...\n",
      "./result/3d_unet_custom_propotional_bce_8_32_class_recon_fold_1/log.csv does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99%|▉| 91/92 [15:07<00:07,  7.07s/it, Epoch=1/10, loss=0.8567, dice_score=0.457[2024-10-30 05:09:12,970] [INFO] [logging.py:96:log_dist] [Rank 0] step=92, skipped=0, lr=[0.00011999999999999996], mom=[(0.8500000000000001, 0.99)]\n",
      "[2024-10-30 05:09:12,972] [INFO] [timer.py:260:stop] epoch=0/micro_step=92/global_step=92, RunningAvgSamplesPerSec=0.8224286372198911, CurrSamplesPerSec=1.8139744840208065, MemAllocated=0.65GB, MaxMemAllocated=20.65GB\n",
      "100%|█| 92/92 [15:11<00:00,  9.91s/it, Epoch=1/10, loss=0.8553, dice_score=0.459\n",
      "100%|███████████████████████████████████████████| 12/12 [09:45<00:00, 48.79s/it]\n",
      "1 - 0.8553 - 0.4595 - 0.7514 - 0.1508 - 1.0000\n",
      " 99%|▉| 91/92 [14:11<00:06,  6.65s/it, Epoch=2/10, loss=0.5882, dice_score=0.726[2024-10-30 05:33:15,758] [INFO] [logging.py:96:log_dist] [Rank 0] step=184, skipped=0, lr=[0.00019999999999999993], mom=[(0.8, 0.99)]\n",
      "[2024-10-30 05:33:15,794] [INFO] [timer.py:260:stop] epoch=0/micro_step=184/global_step=184, RunningAvgSamplesPerSec=0.8413983294181252, CurrSamplesPerSec=1.9831226930405075, MemAllocated=0.65GB, MaxMemAllocated=20.65GB\n",
      "100%|█| 92/92 [14:15<00:00,  9.30s/it, Epoch=2/10, loss=0.5947, dice_score=0.720\n",
      "100%|███████████████████████████████████████████| 12/12 [09:25<00:00, 47.16s/it]\n",
      "2 - 0.5947 - 0.7201 - 0.7201 - 0.0002 - 1.0000\n",
      " 99%|▉| 91/92 [14:16<00:06,  6.79s/it, Epoch=3/10, loss=0.5673, dice_score=0.734[2024-10-30 05:57:04,345] [INFO] [logging.py:96:log_dist] [Rank 0] step=276, skipped=0, lr=[0.000168], mom=[(0.8200000000000001, 0.99)]\n",
      "[2024-10-30 05:57:04,352] [INFO] [timer.py:260:stop] epoch=0/micro_step=276/global_step=276, RunningAvgSamplesPerSec=0.8461976718305931, CurrSamplesPerSec=1.8308618417161362, MemAllocated=0.65GB, MaxMemAllocated=20.65GB\n",
      "100%|█| 92/92 [14:20<00:00,  9.36s/it, Epoch=3/10, loss=0.5709, dice_score=0.732\n",
      "100%|███████████████████████████████████████████| 12/12 [09:20<00:00, 46.73s/it]\n",
      "3 - 0.5709 - 0.7323 - 0.7323 - 0.0001 - 1.0000\n",
      " 99%|▉| 91/92 [14:26<00:06,  6.44s/it, Epoch=4/10, loss=0.5639, dice_score=0.725[2024-10-30 06:20:57,630] [INFO] [logging.py:96:log_dist] [Rank 0] step=368, skipped=0, lr=[0.000136], mom=[(0.8400000000000001, 0.99)]\n",
      "[2024-10-30 06:20:57,651] [INFO] [timer.py:260:stop] epoch=0/micro_step=368/global_step=368, RunningAvgSamplesPerSec=0.8461222836257523, CurrSamplesPerSec=1.832526873146689, MemAllocated=0.65GB, MaxMemAllocated=20.65GB\n",
      "100%|█| 92/92 [14:30<00:00,  9.47s/it, Epoch=4/10, loss=0.5665, dice_score=0.722\n",
      "100%|███████████████████████████████████████████| 12/12 [09:10<00:00, 45.90s/it]\n",
      "4 - 0.5665 - 0.7228 - 0.7228 - 0.0000 - 1.0000\n",
      " 99%|▉| 91/92 [14:24<00:07,  7.47s/it, Epoch=5/10, loss=0.5158, dice_score=0.730[2024-10-30 06:44:39,225] [INFO] [logging.py:96:log_dist] [Rank 0] step=460, skipped=0, lr=[0.00010399999999999998], mom=[(0.8600000000000001, 0.99)]\n",
      "[2024-10-30 06:44:39,228] [INFO] [timer.py:260:stop] epoch=0/micro_step=460/global_step=460, RunningAvgSamplesPerSec=0.8464493337396884, CurrSamplesPerSec=1.8655242011593411, MemAllocated=0.65GB, MaxMemAllocated=20.65GB\n",
      "100%|█| 92/92 [14:29<00:00,  9.45s/it, Epoch=5/10, loss=0.5196, dice_score=0.728\n",
      "100%|███████████████████████████████████████████| 12/12 [09:27<00:00, 47.32s/it]\n",
      "5 - 0.5196 - 0.7283 - 0.7446 - 0.0000 - 1.0000\n",
      " 99%|▉| 91/92 [14:17<00:06,  6.73s/it, Epoch=6/10, loss=0.5127, dice_score=0.723[2024-10-30 07:08:31,054] [INFO] [logging.py:96:log_dist] [Rank 0] step=552, skipped=0, lr=[7.199999999999999e-05], mom=[(0.88, 0.99)]\n",
      "[2024-10-30 07:08:31,066] [INFO] [timer.py:260:stop] epoch=0/micro_step=552/global_step=552, RunningAvgSamplesPerSec=0.8477879446289212, CurrSamplesPerSec=1.8913154150874907, MemAllocated=0.65GB, MaxMemAllocated=20.65GB\n",
      "100%|█| 92/92 [14:22<00:00,  9.37s/it, Epoch=6/10, loss=0.5129, dice_score=0.722\n",
      "100%|███████████████████████████████████████████| 12/12 [09:05<00:00, 45.43s/it]\n",
      "6 - 0.5129 - 0.7228 - 0.7473 - 0.0000 - 1.0000\n",
      " 99%|▉| 91/92 [14:19<00:07,  7.28s/it, Epoch=7/10, loss=0.4993, dice_score=0.715[2024-10-30 07:32:02,364] [INFO] [logging.py:96:log_dist] [Rank 0] step=644, skipped=0, lr=[4e-05], mom=[(0.9, 0.99)]\n",
      "[2024-10-30 07:32:02,366] [INFO] [timer.py:260:stop] epoch=0/micro_step=644/global_step=644, RunningAvgSamplesPerSec=0.8484328623379653, CurrSamplesPerSec=1.907973451113572, MemAllocated=0.65GB, MaxMemAllocated=20.65GB\n",
      "100%|█| 92/92 [14:24<00:00,  9.40s/it, Epoch=7/10, loss=0.5044, dice_score=0.712\n",
      "100%|███████████████████████████████████████████| 12/12 [09:14<00:00, 46.20s/it]\n",
      "7 - 0.5044 - 0.7120 - 0.7514 - 0.0000 - 1.0000\n",
      " 99%|▉| 91/92 [14:21<00:07,  7.36s/it, Epoch=8/10, loss=0.4840, dice_score=0.736[2024-10-30 07:55:44,145] [INFO] [logging.py:96:log_dist] [Rank 0] step=736, skipped=0, lr=[3.692307692307693e-05], mom=[(0.9, 0.99)]\n",
      "[2024-10-30 07:55:44,148] [INFO] [timer.py:260:stop] epoch=0/micro_step=736/global_step=736, RunningAvgSamplesPerSec=0.8487617198541327, CurrSamplesPerSec=1.9117903081216596, MemAllocated=0.65GB, MaxMemAllocated=20.65GB\n",
      "100%|█| 92/92 [14:25<00:00,  9.41s/it, Epoch=8/10, loss=0.4819, dice_score=0.736\n",
      "100%|███████████████████████████████████████████| 12/12 [09:20<00:00, 46.70s/it]\n",
      "8 - 0.4819 - 0.7364 - 0.7690 - 0.0000 - 1.0000\n",
      " 99%|▉| 91/92 [14:26<00:07,  7.27s/it, Epoch=9/10, loss=0.4743, dice_score=0.734[2024-10-30 08:19:37,425] [INFO] [logging.py:96:log_dist] [Rank 0] step=828, skipped=0, lr=[3.4285714285714284e-05], mom=[(0.9, 0.99)]\n",
      "[2024-10-30 08:19:37,426] [INFO] [timer.py:260:stop] epoch=0/micro_step=828/global_step=828, RunningAvgSamplesPerSec=0.8484206722756331, CurrSamplesPerSec=1.7820474704482083, MemAllocated=0.65GB, MaxMemAllocated=20.65GB\n",
      "100%|█| 92/92 [14:31<00:00,  9.47s/it, Epoch=9/10, loss=0.4780, dice_score=0.732\n",
      "100%|███████████████████████████████████████████| 12/12 [09:26<00:00, 47.24s/it]\n",
      "9 - 0.4780 - 0.7323 - 0.7636 - 0.0000 - 1.0000\n",
      " 99%|▉| 91/92 [14:10<00:07,  7.10s/it, Epoch=10/10, loss=0.5025, dice_score=0.71[2024-10-30 08:43:21,914] [INFO] [logging.py:96:log_dist] [Rank 0] step=920, skipped=0, lr=[3.2000000000000005e-05], mom=[(0.9, 0.99)]\n",
      "[2024-10-30 08:43:21,924] [INFO] [timer.py:260:stop] epoch=0/micro_step=920/global_step=920, RunningAvgSamplesPerSec=0.849673158524479, CurrSamplesPerSec=1.8687412468276667, MemAllocated=0.65GB, MaxMemAllocated=20.65GB\n",
      "100%|█| 92/92 [14:15<00:00,  9.30s/it, Epoch=10/10, loss=0.5068, dice_score=0.71\n",
      "100%|███████████████████████████████████████████| 12/12 [09:34<00:00, 37.65s/it]train_valid_test: 733_92_92\n",
      "100%|███████████████████████████████████████████| 12/12 [09:34<00:00, 47.89s/it]\n",
      "train_valid_test: 733_92_92\n",
      "10 - 0.5068 - 0.7160 - 0.7514 - 0.0000 - 1.0000\n",
      "train_valid_test: 733_92_92\n",
      "model_param_num = 24001133\n",
      "model_param_num = 24001133\n",
      "model_param_num = 24001133\n",
      "train_valid_test: 733_92_92\n",
      "model_param_num = 24001133\n",
      "[2024-10-30 08:52:58,939] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.6, git-hash=unknown, git-branch=unknown\n",
      "[2024-10-30 08:52:59,025] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "Adam Optimizer #6 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "Adam Optimizer #6 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "Adam Optimizer #6 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "Adam Optimizer #6 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "[2024-10-30 08:53:00,656] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
      "[2024-10-30 08:53:00,657] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-10-30 08:53:00,686] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "[2024-10-30 08:53:00,686] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "[2024-10-30 08:53:00,686] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer\n",
      "[2024-10-30 08:53:00,686] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 200000000\n",
      "[2024-10-30 08:53:00,686] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 200000000\n",
      "[2024-10-30 08:53:00,686] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: True\n",
      "[2024-10-30 08:53:00,686] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam Optimizer #7 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "Adam Optimizer #7 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "Adam Optimizer #7 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "[2024-10-30 08:53:01,893] [INFO] [utils.py:791:see_memory_usage] Before initializing optimizer states\n",
      "[2024-10-30 08:53:01,894] [INFO] [utils.py:792:see_memory_usage] MA 0.58 GB         Max_MA 0.69 GB         CA 0.87 GB         Max_CA 1 GB \n",
      "[2024-10-30 08:53:01,894] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 77.95 GB, percent = 31.0%\n",
      "[2024-10-30 08:53:02,073] [INFO] [utils.py:791:see_memory_usage] After initializing optimizer states\n",
      "[2024-10-30 08:53:02,073] [INFO] [utils.py:792:see_memory_usage] MA 0.58 GB         Max_MA 0.58 GB         CA 0.87 GB         Max_CA 1 GB \n",
      "[2024-10-30 08:53:02,074] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 77.95 GB, percent = 31.0%\n",
      "[2024-10-30 08:53:02,074] [INFO] [stage_1_and_2.py:516:__init__] optimizer state initialized\n",
      "[2024-10-30 08:53:02,233] [INFO] [utils.py:791:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-10-30 08:53:02,234] [INFO] [utils.py:792:see_memory_usage] MA 0.58 GB         Max_MA 0.58 GB         CA 0.87 GB         Max_CA 1 GB \n",
      "[2024-10-30 08:53:02,234] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 77.95 GB, percent = 31.0%\n",
      "[2024-10-30 08:53:02,243] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n",
      "[2024-10-30 08:53:02,244] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = OneCycle\n",
      "[2024-10-30 08:53:02,244] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.OneCycle object at 0x7fc036255d20>\n",
      "[2024-10-30 08:53:02,244] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[4e-05], mom=[(0.8, 0.99)]\n",
      "[2024-10-30 08:53:02,247] [INFO] [config.py:984:print] DeepSpeedEngine configuration:\n",
      "[2024-10-30 08:53:02,247] [INFO] [config.py:988:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-10-30 08:53:02,247] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-10-30 08:53:02,247] [INFO] [config.py:988:print]   amp_enabled .................. False\n",
      "[2024-10-30 08:53:02,247] [INFO] [config.py:988:print]   amp_params ................... False\n",
      "[2024-10-30 08:53:02,247] [INFO] [config.py:988:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-10-30 08:53:02,247] [INFO] [config.py:988:print]   bfloat16_enabled ............. False\n",
      "[2024-10-30 08:53:02,247] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-10-30 08:53:02,247] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-10-30 08:53:02,247] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-10-30 08:53:02,247] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc036255690>\n",
      "[2024-10-30 08:53:02,247] [INFO] [config.py:988:print]   communication_data_type ...... None\n",
      "[2024-10-30 08:53:02,247] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-10-30 08:53:02,247] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False\n",
      "[2024-10-30 08:53:02,247] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   dataloader_drop_last ......... False\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   disable_allgather ............ False\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   dump_state ................... False\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   elasticity_enabled ........... False\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   fp16_auto_cast ............... None\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   fp16_enabled ................. False\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   global_rank .................. 0\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   grad_accum_dtype ............. None\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   gradient_clipping ............ 1.0\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   graph_harvesting ............. False\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 65536\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   load_universal_checkpoint .... False\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   loss_scale ................... 0\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   memory_breakdown ............. False\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   mics_shard_size .............. -1\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   optimizer_name ............... adam\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   optimizer_params ............. {'lr': 1e-05, 'betas': [0.9, 0.999]}\n",
      "[2024-10-30 08:53:02,248] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-10-30 08:53:02,249] [INFO] [config.py:988:print]   pld_enabled .................. False\n",
      "[2024-10-30 08:53:02,249] [INFO] [config.py:988:print]   pld_params ................... False\n",
      "[2024-10-30 08:53:02,249] [INFO] [config.py:988:print]   prescale_gradients ........... False\n",
      "[2024-10-30 08:53:02,249] [INFO] [config.py:988:print]   scheduler_name ............... OneCycle\n",
      "[2024-10-30 08:53:02,249] [INFO] [config.py:988:print]   scheduler_params ............. {'cycle_min_lr': 4e-05, 'cycle_max_lr': 0.0002, 'decay_lr_rate': 0.25, 'cycle_first_step_size': 184, 'cycle_second_step_size': 460, 'decay_step_size': 276}\n",
      "[2024-10-30 08:53:02,249] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-10-30 08:53:02,249] [INFO] [config.py:988:print]   sparse_attention ............. None\n",
      "[2024-10-30 08:53:02,249] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False\n",
      "[2024-10-30 08:53:02,249] [INFO] [config.py:988:print]   steps_per_print .............. 92\n",
      "[2024-10-30 08:53:02,249] [INFO] [config.py:988:print]   train_batch_size ............. 8\n",
      "[2024-10-30 08:53:02,249] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  2\n",
      "[2024-10-30 08:53:02,249] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False\n",
      "[2024-10-30 08:53:02,249] [INFO] [config.py:988:print]   use_node_local_storage ....... False\n",
      "[2024-10-30 08:53:02,249] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False\n",
      "[2024-10-30 08:53:02,249] [INFO] [config.py:988:print]   weight_quantization_config ... None\n",
      "[2024-10-30 08:53:02,249] [INFO] [config.py:988:print]   world_size ................... 4\n",
      "[2024-10-30 08:53:02,249] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False\n",
      "[2024-10-30 08:53:02,249] [INFO] [config.py:988:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=PosixPath('/local_nvme'), buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=PosixPath('/local_nvme'), buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-10-30 08:53:02,249] [INFO] [config.py:988:print]   zero_enabled ................. True\n",
      "[2024-10-30 08:53:02,249] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-10-30 08:53:02,249] [INFO] [config.py:988:print]   zero_optimization_stage ...... 2\n",
      "[2024-10-30 08:53:02,249] [INFO] [config.py:974:print_user_config]   json = {\n",
      "    \"train_batch_size\": 8, \n",
      "    \"train_micro_batch_size_per_gpu\": 2, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"pipeline_parallel\": true, \n",
      "    \"steps_per_print\": 92, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"hysteresis\": 2, \n",
      "        \"consecutive_hysteresis\": false, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"nvme_path\": \"/local_nvme\", \n",
      "            \"pin_memory\": true, \n",
      "            \"buffer_count\": 4, \n",
      "            \"fast_init\": false\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"nvme_path\": \"/local_nvme\", \n",
      "            \"pin_memory\": true, \n",
      "            \"buffer_count\": 5, \n",
      "            \"buffer_size\": 1.000000e+08, \n",
      "            \"max_in_cpu\": 1.000000e+09\n",
      "        }\n",
      "    }, \n",
      "    \"data_sampling\": {\n",
      "        \"enabled\": true, \n",
      "        \"num_workers\": 16\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"OneCycle\", \n",
      "        \"params\": {\n",
      "            \"cycle_min_lr\": 4e-05, \n",
      "            \"cycle_max_lr\": 0.0002, \n",
      "            \"decay_lr_rate\": 0.25, \n",
      "            \"cycle_first_step_size\": 184, \n",
      "            \"cycle_second_step_size\": 460, \n",
      "            \"decay_step_size\": 276\n",
      "        }\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"Adam\", \n",
      "        \"params\": {\n",
      "            \"lr\": 1e-05, \n",
      "            \"betas\": [0.9, 0.999]\n",
      "        }\n",
      "    }\n",
      "}\n",
      "[2024-10-30 08:53:02,249] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.6, git-hash=unknown, git-branch=unknown\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-30 08:53:02,293] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "Adam Optimizer #7 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "[2024-10-30 08:53:03,919] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
      "[2024-10-30 08:53:03,920] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-10-30 08:53:03,948] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "[2024-10-30 08:53:03,949] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "[2024-10-30 08:53:03,949] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer\n",
      "[2024-10-30 08:53:03,949] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 200000000\n",
      "[2024-10-30 08:53:03,949] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 200000000\n",
      "[2024-10-30 08:53:03,949] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: True\n",
      "[2024-10-30 08:53:03,949] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False\n",
      "Adam Optimizer #8 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "[2024-10-30 08:53:04,262] [INFO] [utils.py:791:see_memory_usage] Before initializing optimizer states\n",
      "[2024-10-30 08:53:04,269] [INFO] [utils.py:792:see_memory_usage] MA 0.69 GB         Max_MA 0.69 GB         CA 0.87 GB         Max_CA 1 GB \n",
      "[2024-10-30 08:53:04,270] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 81.87 GB, percent = 32.5%\n",
      "Adam Optimizer #8 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "Adam Optimizer #8 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "[2024-10-30 08:53:04,545] [INFO] [utils.py:791:see_memory_usage] After initializing optimizer states\n",
      "[2024-10-30 08:53:04,546] [INFO] [utils.py:792:see_memory_usage] MA 0.69 GB         Max_MA 0.69 GB         CA 0.87 GB         Max_CA 1 GB \n",
      "[2024-10-30 08:53:04,546] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 82.13 GB, percent = 32.7%\n",
      "[2024-10-30 08:53:04,546] [INFO] [stage_1_and_2.py:516:__init__] optimizer state initialized\n",
      "[2024-10-30 08:53:04,802] [INFO] [utils.py:791:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-10-30 08:53:04,803] [INFO] [utils.py:792:see_memory_usage] MA 0.69 GB         Max_MA 0.69 GB         CA 0.87 GB         Max_CA 1 GB \n",
      "[2024-10-30 08:53:04,803] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 82.13 GB, percent = 32.7%\n",
      "[2024-10-30 08:53:04,817] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n",
      "[2024-10-30 08:53:04,817] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = OneCycle\n",
      "[2024-10-30 08:53:04,817] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.OneCycle object at 0x7fc0366264d0>\n",
      "[2024-10-30 08:53:04,817] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[4e-05], mom=[(0.8, 0.99)]\n",
      "[2024-10-30 08:53:04,822] [INFO] [config.py:984:print] DeepSpeedEngine configuration:\n",
      "[2024-10-30 08:53:04,822] [INFO] [config.py:988:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-10-30 08:53:04,822] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-10-30 08:53:04,822] [INFO] [config.py:988:print]   amp_enabled .................. False\n",
      "[2024-10-30 08:53:04,822] [INFO] [config.py:988:print]   amp_params ................... False\n",
      "[2024-10-30 08:53:04,822] [INFO] [config.py:988:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-10-30 08:53:04,822] [INFO] [config.py:988:print]   bfloat16_enabled ............. False\n",
      "[2024-10-30 08:53:04,822] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-10-30 08:53:04,822] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-10-30 08:53:04,822] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-10-30 08:53:04,822] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc036255840>\n",
      "[2024-10-30 08:53:04,823] [INFO] [config.py:988:print]   communication_data_type ...... None\n",
      "[2024-10-30 08:53:04,823] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-10-30 08:53:04,823] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False\n",
      "[2024-10-30 08:53:04,823] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False\n",
      "[2024-10-30 08:53:04,823] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-10-30 08:53:04,823] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False\n",
      "[2024-10-30 08:53:04,823] [INFO] [config.py:988:print]   dataloader_drop_last ......... False\n",
      "[2024-10-30 08:53:04,823] [INFO] [config.py:988:print]   disable_allgather ............ False\n",
      "[2024-10-30 08:53:04,823] [INFO] [config.py:988:print]   dump_state ................... False\n",
      "[2024-10-30 08:53:04,823] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-10-30 08:53:04,823] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False\n",
      "[2024-10-30 08:53:04,823] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-10-30 08:53:04,823] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-10-30 08:53:04,823] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-10-30 08:53:04,823] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-10-30 08:53:04,823] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-10-30 08:53:04,823] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-10-30 08:53:04,823] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False\n",
      "[2024-10-30 08:53:04,823] [INFO] [config.py:988:print]   elasticity_enabled ........... False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-30 08:53:04,823] [INFO] [config.py:988:print]   flops_profiler_config ........ {\r\n",
      "    \"enabled\": false, \r\n",
      "    \"recompute_fwd_factor\": 0.0, \r\n",
      "    \"profile_step\": 1, \r\n",
      "    \"module_depth\": -1, \r\n",
      "    \"top_modules\": 1, \r\n",
      "    \"detailed\": true, \r\n",
      "    \"output_file\": null\r\n",
      "}\r\n",
      "[2024-10-30 08:53:04,823] [INFO] [config.py:988:print]   fp16_auto_cast ............... None\r\n",
      "[2024-10-30 08:53:04,823] [INFO] [config.py:988:print]   fp16_enabled ................. False\r\n",
      "[2024-10-30 08:53:04,823] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False\r\n",
      "[2024-10-30 08:53:04,823] [INFO] [config.py:988:print]   global_rank .................. 0\r\n",
      "[2024-10-30 08:53:04,823] [INFO] [config.py:988:print]   grad_accum_dtype ............. None\r\n",
      "[2024-10-30 08:53:04,823] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1\r\n",
      "[2024-10-30 08:53:04,823] [INFO] [config.py:988:print]   gradient_clipping ............ 1.0\r\n",
      "[2024-10-30 08:53:04,823] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0\r\n",
      "[2024-10-30 08:53:04,823] [INFO] [config.py:988:print]   graph_harvesting ............. False\r\n",
      "[2024-10-30 08:53:04,823] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\r\n",
      "[2024-10-30 08:53:04,823] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 65536\r\n",
      "[2024-10-30 08:53:04,824] [INFO] [config.py:988:print]   load_universal_checkpoint .... False\r\n",
      "[2024-10-30 08:53:04,824] [INFO] [config.py:988:print]   loss_scale ................... 0\r\n",
      "[2024-10-30 08:53:04,824] [INFO] [config.py:988:print]   memory_breakdown ............. False\r\n",
      "[2024-10-30 08:53:04,824] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False\r\n",
      "[2024-10-30 08:53:04,824] [INFO] [config.py:988:print]   mics_shard_size .............. -1\r\n",
      "[2024-10-30 08:53:04,824] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\r\n",
      "[2024-10-30 08:53:04,824] [INFO] [config.py:988:print]   nebula_config ................ {\r\n",
      "    \"enabled\": false, \r\n",
      "    \"persistent_storage_path\": null, \r\n",
      "    \"persistent_time_interval\": 100, \r\n",
      "    \"num_of_version_in_retention\": 2, \r\n",
      "    \"enable_nebula_load\": true, \r\n",
      "    \"load_path\": null\r\n",
      "}\r\n",
      "[2024-10-30 08:53:04,824] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False\r\n",
      "[2024-10-30 08:53:04,824] [INFO] [config.py:988:print]   optimizer_name ............... adam\r\n",
      "[2024-10-30 08:53:04,824] [INFO] [config.py:988:print]   optimizer_params ............. {'lr': 1e-05, 'betas': [0.9, 0.999]}\r\n",
      "[2024-10-30 08:53:04,824] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\r\n",
      "[2024-10-30 08:53:04,824] [INFO] [config.py:988:print]   pld_enabled .................. False\r\n",
      "[2024-10-30 08:53:04,824] [INFO] [config.py:988:print]   pld_params ................... False\r\n",
      "[2024-10-30 08:53:04,824] [INFO] [config.py:988:print]   prescale_gradients ........... False\r\n",
      "[2024-10-30 08:53:04,824] [INFO] [config.py:988:print]   scheduler_name ............... OneCycle\r\n",
      "[2024-10-30 08:53:04,824] [INFO] [config.py:988:print]   scheduler_params ............. {'cycle_min_lr': 4e-05, 'cycle_max_lr': 0.0002, 'decay_lr_rate': 0.25, 'cycle_first_step_size': 184, 'cycle_second_step_size': 460, 'decay_step_size': 276}\r\n",
      "[2024-10-30 08:53:04,824] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32\r\n",
      "[2024-10-30 08:53:04,824] [INFO] [config.py:988:print]   sparse_attention ............. None\r\n",
      "[2024-10-30 08:53:04,824] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False\r\n",
      "[2024-10-30 08:53:04,824] [INFO] [config.py:988:print]   steps_per_print .............. 92\r\n",
      "[2024-10-30 08:53:04,824] [INFO] [config.py:988:print]   train_batch_size ............. 8\r\n",
      "[2024-10-30 08:53:04,824] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  2\r\n",
      "[2024-10-30 08:53:04,824] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False\r\n",
      "[2024-10-30 08:53:04,824] [INFO] [config.py:988:print]   use_node_local_storage ....... False\r\n",
      "[2024-10-30 08:53:04,824] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False\r\n",
      "[2024-10-30 08:53:04,824] [INFO] [config.py:988:print]   weight_quantization_config ... None\r\n",
      "[2024-10-30 08:53:04,824] [INFO] [config.py:988:print]   world_size ................... 4\r\n",
      "[2024-10-30 08:53:04,824] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False\r\n",
      "[2024-10-30 08:53:04,824] [INFO] [config.py:988:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=PosixPath('/local_nvme'), buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=PosixPath('/local_nvme'), buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\r\n",
      "[2024-10-30 08:53:04,824] [INFO] [config.py:988:print]   zero_enabled ................. True\r\n",
      "[2024-10-30 08:53:04,824] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True\r\n",
      "[2024-10-30 08:53:04,824] [INFO] [config.py:988:print]   zero_optimization_stage ...... 2\r\n",
      "[2024-10-30 08:53:04,825] [INFO] [config.py:974:print_user_config]   json = {\r\n",
      "    \"train_batch_size\": 8, \r\n",
      "    \"train_micro_batch_size_per_gpu\": 2, \r\n",
      "    \"gradient_clipping\": 1.0, \r\n",
      "    \"pipeline_parallel\": true, \r\n",
      "    \"steps_per_print\": 92, \r\n",
      "    \"fp16\": {\r\n",
      "        \"enabled\": false, \r\n",
      "        \"loss_scale\": 0, \r\n",
      "        \"loss_scale_window\": 1000, \r\n",
      "        \"hysteresis\": 2, \r\n",
      "        \"consecutive_hysteresis\": false, \r\n",
      "        \"min_loss_scale\": 1\r\n",
      "    }, \r\n",
      "    \"bf16\": {\r\n",
      "        \"enabled\": false\r\n",
      "    }, \r\n",
      "    \"zero_optimization\": {\r\n",
      "        \"stage\": 2, \r\n",
      "        \"allgather_partitions\": true, \r\n",
      "        \"allgather_bucket_size\": 2.000000e+08, \r\n",
      "        \"reduce_scatter\": true, \r\n",
      "        \"reduce_bucket_size\": 2.000000e+08, \r\n",
      "        \"overlap_comm\": true, \r\n",
      "        \"contiguous_gradients\": true, \r\n",
      "        \"offload_optimizer\": {\r\n",
      "            \"device\": \"cpu\", \r\n",
      "            \"nvme_path\": \"/local_nvme\", \r\n",
      "            \"pin_memory\": true, \r\n",
      "            \"buffer_count\": 4, \r\n",
      "            \"fast_init\": false\r\n",
      "        }, \r\n",
      "        \"offload_param\": {\r\n",
      "            \"device\": \"cpu\", \r\n",
      "            \"nvme_path\": \"/local_nvme\", \r\n",
      "            \"pin_memory\": true, \r\n",
      "            \"buffer_count\": 5, \r\n",
      "            \"buffer_size\": 1.000000e+08, \r\n",
      "            \"max_in_cpu\": 1.000000e+09\r\n",
      "        }\r\n",
      "    }, \r\n",
      "    \"data_sampling\": {\r\n",
      "        \"enabled\": true, \r\n",
      "        \"num_workers\": 16\r\n",
      "    }, \r\n",
      "    \"scheduler\": {\r\n",
      "        \"type\": \"OneCycle\", \r\n",
      "        \"params\": {\r\n",
      "            \"cycle_min_lr\": 4e-05, \r\n",
      "            \"cycle_max_lr\": 0.0002, \r\n",
      "            \"decay_lr_rate\": 0.25, \r\n",
      "            \"cycle_first_step_size\": 184, \r\n",
      "            \"cycle_second_step_size\": 460, \r\n",
      "            \"decay_step_size\": 276\r\n",
      "        }\r\n",
      "    }, \r\n",
      "    \"optimizer\": {\r\n",
      "        \"type\": \"Adam\", \r\n",
      "        \"params\": {\r\n",
      "            \"lr\": 1e-05, \r\n",
      "            \"betas\": [0.9, 0.999]\r\n",
      "        }\r\n",
      "    }\r\n",
      "}\r\n",
      "[2024-10-30 08:53:04,825] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.6, git-hash=unknown, git-branch=unknown\r\n",
      "[2024-10-30 08:53:04,877] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "Adam Optimizer #8 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "[2024-10-30 08:53:06,544] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
      "[2024-10-30 08:53:06,544] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-10-30 08:53:06,572] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "[2024-10-30 08:53:06,572] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "[2024-10-30 08:53:06,572] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer\n",
      "[2024-10-30 08:53:06,572] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 200000000\n",
      "[2024-10-30 08:53:06,572] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 200000000\n",
      "[2024-10-30 08:53:06,572] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: True\n",
      "[2024-10-30 08:53:06,572] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False\n",
      "[2024-10-30 08:53:07,097] [INFO] [utils.py:791:see_memory_usage] Before initializing optimizer states\n",
      "[2024-10-30 08:53:07,099] [INFO] [utils.py:792:see_memory_usage] MA 0.69 GB         Max_MA 0.79 GB         CA 0.96 GB         Max_CA 1 GB \n",
      "[2024-10-30 08:53:07,099] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 82.49 GB, percent = 32.8%\n",
      "[2024-10-30 08:53:07,310] [INFO] [utils.py:791:see_memory_usage] After initializing optimizer states\n",
      "[2024-10-30 08:53:07,311] [INFO] [utils.py:792:see_memory_usage] MA 0.69 GB         Max_MA 0.69 GB         CA 0.96 GB         Max_CA 1 GB \n",
      "[2024-10-30 08:53:07,312] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 82.6 GB, percent = 32.8%\n",
      "[2024-10-30 08:53:07,312] [INFO] [stage_1_and_2.py:516:__init__] optimizer state initialized\n",
      "[2024-10-30 08:53:07,520] [INFO] [utils.py:791:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-10-30 08:53:07,521] [INFO] [utils.py:792:see_memory_usage] MA 0.69 GB         Max_MA 0.69 GB         CA 0.96 GB         Max_CA 1 GB \n",
      "[2024-10-30 08:53:07,521] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 82.66 GB, percent = 32.9%\n",
      "[2024-10-30 08:53:07,536] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n",
      "[2024-10-30 08:53:07,536] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = OneCycle\n",
      "[2024-10-30 08:53:07,536] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.OneCycle object at 0x7fc0363dffa0>\n",
      "[2024-10-30 08:53:07,536] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[4e-05], mom=[(0.8, 0.99)]\n",
      "[2024-10-30 08:53:07,539] [INFO] [config.py:984:print] DeepSpeedEngine configuration:\n",
      "[2024-10-30 08:53:07,540] [INFO] [config.py:988:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-10-30 08:53:07,540] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-10-30 08:53:07,540] [INFO] [config.py:988:print]   amp_enabled .................. False\n",
      "[2024-10-30 08:53:07,540] [INFO] [config.py:988:print]   amp_params ................... False\n",
      "[2024-10-30 08:53:07,540] [INFO] [config.py:988:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-10-30 08:53:07,540] [INFO] [config.py:988:print]   bfloat16_enabled ............. False\n",
      "[2024-10-30 08:53:07,540] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-10-30 08:53:07,540] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-10-30 08:53:07,540] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-10-30 08:53:07,540] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fbffc0338e0>\n",
      "[2024-10-30 08:53:07,540] [INFO] [config.py:988:print]   communication_data_type ...... None\n",
      "[2024-10-30 08:53:07,540] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-10-30 08:53:07,540] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False\n",
      "[2024-10-30 08:53:07,540] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False\n",
      "[2024-10-30 08:53:07,540] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-10-30 08:53:07,540] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False\n",
      "[2024-10-30 08:53:07,540] [INFO] [config.py:988:print]   dataloader_drop_last ......... False\n",
      "[2024-10-30 08:53:07,540] [INFO] [config.py:988:print]   disable_allgather ............ False\n",
      "[2024-10-30 08:53:07,540] [INFO] [config.py:988:print]   dump_state ................... False\n",
      "[2024-10-30 08:53:07,540] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-10-30 08:53:07,541] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False\n",
      "[2024-10-30 08:53:07,541] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-10-30 08:53:07,541] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-10-30 08:53:07,541] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-10-30 08:53:07,541] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-10-30 08:53:07,541] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-10-30 08:53:07,541] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-10-30 08:53:07,541] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False\n",
      "[2024-10-30 08:53:07,541] [INFO] [config.py:988:print]   elasticity_enabled ........... False\n",
      "[2024-10-30 08:53:07,541] [INFO] [config.py:988:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-10-30 08:53:07,541] [INFO] [config.py:988:print]   fp16_auto_cast ............... None\n",
      "[2024-10-30 08:53:07,541] [INFO] [config.py:988:print]   fp16_enabled ................. False\n",
      "[2024-10-30 08:53:07,541] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-10-30 08:53:07,541] [INFO] [config.py:988:print]   global_rank .................. 0\n",
      "[2024-10-30 08:53:07,541] [INFO] [config.py:988:print]   grad_accum_dtype ............. None\n",
      "[2024-10-30 08:53:07,541] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1\n",
      "[2024-10-30 08:53:07,541] [INFO] [config.py:988:print]   gradient_clipping ............ 1.0\n",
      "[2024-10-30 08:53:07,541] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-10-30 08:53:07,541] [INFO] [config.py:988:print]   graph_harvesting ............. False\n",
      "[2024-10-30 08:53:07,541] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-10-30 08:53:07,541] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 65536\n",
      "[2024-10-30 08:53:07,541] [INFO] [config.py:988:print]   load_universal_checkpoint .... False\n",
      "[2024-10-30 08:53:07,541] [INFO] [config.py:988:print]   loss_scale ................... 0\n",
      "[2024-10-30 08:53:07,541] [INFO] [config.py:988:print]   memory_breakdown ............. False\n",
      "[2024-10-30 08:53:07,541] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False\n",
      "[2024-10-30 08:53:07,541] [INFO] [config.py:988:print]   mics_shard_size .............. -1\n",
      "[2024-10-30 08:53:07,541] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-10-30 08:53:07,541] [INFO] [config.py:988:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-10-30 08:53:07,541] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-10-30 08:53:07,541] [INFO] [config.py:988:print]   optimizer_name ............... adam\n",
      "[2024-10-30 08:53:07,541] [INFO] [config.py:988:print]   optimizer_params ............. {'lr': 1e-05, 'betas': [0.9, 0.999]}\n",
      "[2024-10-30 08:53:07,542] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-10-30 08:53:07,542] [INFO] [config.py:988:print]   pld_enabled .................. False\n",
      "[2024-10-30 08:53:07,542] [INFO] [config.py:988:print]   pld_params ................... False\n",
      "[2024-10-30 08:53:07,542] [INFO] [config.py:988:print]   prescale_gradients ........... False\n",
      "[2024-10-30 08:53:07,542] [INFO] [config.py:988:print]   scheduler_name ............... OneCycle\n",
      "[2024-10-30 08:53:07,542] [INFO] [config.py:988:print]   scheduler_params ............. {'cycle_min_lr': 4e-05, 'cycle_max_lr': 0.0002, 'decay_lr_rate': 0.25, 'cycle_first_step_size': 184, 'cycle_second_step_size': 460, 'decay_step_size': 276}\n",
      "[2024-10-30 08:53:07,542] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-10-30 08:53:07,542] [INFO] [config.py:988:print]   sparse_attention ............. None\n",
      "[2024-10-30 08:53:07,542] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False\n",
      "[2024-10-30 08:53:07,542] [INFO] [config.py:988:print]   steps_per_print .............. 92\n",
      "[2024-10-30 08:53:07,542] [INFO] [config.py:988:print]   train_batch_size ............. 8\n",
      "[2024-10-30 08:53:07,542] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  2\n",
      "[2024-10-30 08:53:07,542] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False\n",
      "[2024-10-30 08:53:07,542] [INFO] [config.py:988:print]   use_node_local_storage ....... False\n",
      "[2024-10-30 08:53:07,542] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False\n",
      "[2024-10-30 08:53:07,542] [INFO] [config.py:988:print]   weight_quantization_config ... None\n",
      "[2024-10-30 08:53:07,542] [INFO] [config.py:988:print]   world_size ................... 4\n",
      "[2024-10-30 08:53:07,542] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False\n",
      "[2024-10-30 08:53:07,542] [INFO] [config.py:988:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=PosixPath('/local_nvme'), buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=PosixPath('/local_nvme'), buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-10-30 08:53:07,542] [INFO] [config.py:988:print]   zero_enabled ................. True\n",
      "[2024-10-30 08:53:07,542] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-10-30 08:53:07,542] [INFO] [config.py:988:print]   zero_optimization_stage ...... 2\n",
      "[2024-10-30 08:53:07,543] [INFO] [config.py:974:print_user_config]   json = {\n",
      "    \"train_batch_size\": 8, \n",
      "    \"train_micro_batch_size_per_gpu\": 2, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"pipeline_parallel\": true, \n",
      "    \"steps_per_print\": 92, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"hysteresis\": 2, \n",
      "        \"consecutive_hysteresis\": false, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"nvme_path\": \"/local_nvme\", \n",
      "            \"pin_memory\": true, \n",
      "            \"buffer_count\": 4, \n",
      "            \"fast_init\": false\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"nvme_path\": \"/local_nvme\", \n",
      "            \"pin_memory\": true, \n",
      "            \"buffer_count\": 5, \n",
      "            \"buffer_size\": 1.000000e+08, \n",
      "            \"max_in_cpu\": 1.000000e+09\n",
      "        }\n",
      "    }, \n",
      "    \"data_sampling\": {\n",
      "        \"enabled\": true, \n",
      "        \"num_workers\": 16\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"OneCycle\", \n",
      "        \"params\": {\n",
      "            \"cycle_min_lr\": 4e-05, \n",
      "            \"cycle_max_lr\": 0.0002, \n",
      "            \"decay_lr_rate\": 0.25, \n",
      "            \"cycle_first_step_size\": 184, \n",
      "            \"cycle_second_step_size\": 460, \n",
      "            \"decay_step_size\": 276\n",
      "        }\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"Adam\", \n",
      "        \"params\": {\n",
      "            \"lr\": 1e-05, \n",
      "            \"betas\": [0.9, 0.999]\n",
      "        }\n",
      "    }\n",
      "}\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./result/3d_unet_custom_propotional_bce_8_32_class_recon_fold_2/log.csv check exist...\n",
      "./result/3d_unet_custom_propotional_bce_8_32_class_recon_fold_2/log.csv does not exist.\n",
      " 99%|▉| 91/92 [15:04<00:06,  6.94s/it, Epoch=1/10, loss=0.8938, dice_score=0.443[2024-10-30 09:08:16,129] [INFO] [logging.py:96:log_dist] [Rank 0] step=92, skipped=0, lr=[0.00011999999999999996], mom=[(0.8500000000000001, 0.99)]\n",
      "[2024-10-30 09:08:16,135] [INFO] [timer.py:260:stop] epoch=0/micro_step=92/global_step=92, RunningAvgSamplesPerSec=0.8230315012083989, CurrSamplesPerSec=2.096074224834518, MemAllocated=0.76GB, MaxMemAllocated=20.75GB\n",
      "100%|█| 92/92 [15:08<00:00,  9.88s/it, Epoch=1/10, loss=0.8889, dice_score=0.448\n",
      "100%|███████████████████████████████████████████| 12/12 [12:19<00:00, 61.64s/it]\n",
      "1 - 0.8889 - 0.4486 - 0.7024 - 0.1399 - 1.0000\n",
      " 99%|▉| 91/92 [15:00<00:07,  7.08s/it, Epoch=2/10, loss=0.5860, dice_score=0.729[2024-10-30 09:35:42,124] [INFO] [logging.py:96:log_dist] [Rank 0] step=184, skipped=0, lr=[0.00019999999999999993], mom=[(0.8, 0.99)]\n",
      "[2024-10-30 09:35:42,126] [INFO] [timer.py:260:stop] epoch=0/micro_step=184/global_step=184, RunningAvgSamplesPerSec=0.8186437631063719, CurrSamplesPerSec=1.8609811615935792, MemAllocated=0.76GB, MaxMemAllocated=20.76GB\n",
      "100%|█| 92/92 [15:04<00:00,  9.83s/it, Epoch=2/10, loss=0.5874, dice_score=0.726\n",
      "100%|███████████████████████████████████████████| 12/12 [12:40<00:00, 63.36s/it]\n",
      "2 - 0.5874 - 0.7269 - 0.7269 - 0.0002 - 1.0000\n",
      " 99%|▉| 91/92 [14:54<00:07,  7.31s/it, Epoch=3/10, loss=0.5642, dice_score=0.726[2024-10-30 10:03:23,635] [INFO] [logging.py:96:log_dist] [Rank 0] step=276, skipped=0, lr=[0.000168], mom=[(0.8200000000000001, 0.99)]\n",
      "[2024-10-30 10:03:23,663] [INFO] [timer.py:260:stop] epoch=0/micro_step=276/global_step=276, RunningAvgSamplesPerSec=0.8188572743208019, CurrSamplesPerSec=1.8359534299561104, MemAllocated=0.76GB, MaxMemAllocated=20.76GB\n",
      "100%|█| 92/92 [14:59<00:00,  9.78s/it, Epoch=3/10, loss=0.5615, dice_score=0.728\n",
      "100%|███████████████████████████████████████████| 12/12 [12:22<00:00, 61.91s/it]\n",
      "3 - 0.5615 - 0.7283 - 0.7283 - 0.0001 - 1.0000\n",
      " 99%|▉| 91/92 [14:47<00:07,  7.02s/it, Epoch=4/10, loss=0.5166, dice_score=0.741[2024-10-30 10:30:40,880] [INFO] [logging.py:96:log_dist] [Rank 0] step=368, skipped=0, lr=[0.000136], mom=[(0.8400000000000001, 0.99)]\n",
      "[2024-10-30 10:30:40,886] [INFO] [timer.py:260:stop] epoch=0/micro_step=368/global_step=368, RunningAvgSamplesPerSec=0.8205055437582502, CurrSamplesPerSec=1.834565482740003, MemAllocated=0.76GB, MaxMemAllocated=20.76GB\n",
      "100%|█| 92/92 [14:52<00:00,  9.70s/it, Epoch=4/10, loss=0.5204, dice_score=0.739\n",
      "100%|███████████████████████████████████████████| 12/12 [12:05<00:00, 60.44s/it]\n",
      "4 - 0.5204 - 0.7391 - 0.7459 - 0.0000 - 1.0000\n",
      " 99%|▉| 91/92 [14:46<00:06,  6.65s/it, Epoch=5/10, loss=0.5237, dice_score=0.732[2024-10-30 10:57:39,315] [INFO] [logging.py:96:log_dist] [Rank 0] step=460, skipped=0, lr=[0.00010399999999999998], mom=[(0.8600000000000001, 0.99)]\n",
      "[2024-10-30 10:57:39,317] [INFO] [timer.py:260:stop] epoch=0/micro_step=460/global_step=460, RunningAvgSamplesPerSec=0.8217536826831188, CurrSamplesPerSec=2.0406137355363616, MemAllocated=0.76GB, MaxMemAllocated=20.76GB\n",
      "100%|█| 92/92 [14:51<00:00,  9.69s/it, Epoch=5/10, loss=0.5243, dice_score=0.732\n",
      "100%|███████████████████████████████████████████| 12/12 [12:01<00:00, 60.10s/it]\n",
      "5 - 0.5243 - 0.7323 - 0.7541 - 0.0000 - 1.0000\n",
      " 99%|▉| 91/92 [14:34<00:06,  6.73s/it, Epoch=6/10, loss=0.5119, dice_score=0.726[2024-10-30 11:24:20,889] [INFO] [logging.py:96:log_dist] [Rank 0] step=552, skipped=0, lr=[7.199999999999999e-05], mom=[(0.88, 0.99)]\n",
      "[2024-10-30 11:24:20,897] [INFO] [timer.py:260:stop] epoch=0/micro_step=552/global_step=552, RunningAvgSamplesPerSec=0.8245252913148132, CurrSamplesPerSec=2.033035892915633, MemAllocated=0.76GB, MaxMemAllocated=20.76GB\n",
      "100%|█| 92/92 [14:38<00:00,  9.55s/it, Epoch=6/10, loss=0.5132, dice_score=0.726\n",
      "100%|███████████████████████████████████████████| 12/12 [12:27<00:00, 62.29s/it]\n",
      "6 - 0.5132 - 0.7269 - 0.7636 - 0.0000 - 1.0000\n",
      " 99%|▉| 91/92 [14:37<00:06,  6.40s/it, Epoch=7/10, loss=0.4795, dice_score=0.741[2024-10-30 11:51:31,495] [INFO] [logging.py:96:log_dist] [Rank 0] step=644, skipped=0, lr=[4e-05], mom=[(0.9, 0.99)]\n",
      "[2024-10-30 11:51:31,539] [INFO] [timer.py:260:stop] epoch=0/micro_step=644/global_step=644, RunningAvgSamplesPerSec=0.8261001181384225, CurrSamplesPerSec=1.9993255029710044, MemAllocated=0.76GB, MaxMemAllocated=20.76GB\n",
      "100%|█| 92/92 [14:41<00:00,  9.58s/it, Epoch=7/10, loss=0.4803, dice_score=0.740\n",
      "100%|███████████████████████████████████████████| 12/12 [11:56<00:00, 59.68s/it]\n",
      "7 - 0.4803 - 0.7405 - 0.7717 - 0.0000 - 1.0000\n",
      " 99%|▉| 91/92 [14:38<00:06,  6.84s/it, Epoch=8/10, loss=0.4798, dice_score=0.721[2024-10-30 12:18:12,994] [INFO] [logging.py:96:log_dist] [Rank 0] step=736, skipped=0, lr=[3.692307692307693e-05], mom=[(0.9, 0.99)]\n",
      "[2024-10-30 12:18:13,034] [INFO] [timer.py:260:stop] epoch=0/micro_step=736/global_step=736, RunningAvgSamplesPerSec=0.8270425196894629, CurrSamplesPerSec=1.7800586869634103, MemAllocated=0.76GB, MaxMemAllocated=20.76GB\n",
      "100%|█| 92/92 [14:43<00:00,  9.61s/it, Epoch=8/10, loss=0.4779, dice_score=0.721\n",
      "100%|███████████████████████████████████████████| 12/12 [12:10<00:00, 60.90s/it]\n",
      "8 - 0.4779 - 0.7215 - 0.7677 - 0.0000 - 1.0000\n",
      " 99%|▉| 91/92 [14:35<00:06,  6.89s/it, Epoch=9/10, loss=0.5047, dice_score=0.719[2024-10-30 12:45:06,683] [INFO] [logging.py:96:log_dist] [Rank 0] step=828, skipped=0, lr=[3.4285714285714284e-05], mom=[(0.9, 0.99)]\n",
      "[2024-10-30 12:45:06,701] [INFO] [timer.py:260:stop] epoch=0/micro_step=828/global_step=828, RunningAvgSamplesPerSec=0.8280897622501309, CurrSamplesPerSec=1.827116015036894, MemAllocated=0.76GB, MaxMemAllocated=20.76GB\n",
      "100%|█| 92/92 [14:40<00:00,  9.57s/it, Epoch=9/10, loss=0.5048, dice_score=0.720\n",
      "100%|███████████████████████████████████████████| 12/12 [11:59<00:00, 59.95s/it]\n",
      "9 - 0.5048 - 0.7201 - 0.7636 - 0.0000 - 1.0000\n",
      " 99%|▉| 91/92 [14:36<00:06,  6.71s/it, Epoch=10/10, loss=0.5036, dice_score=0.73[2024-10-30 13:11:49,284] [INFO] [logging.py:96:log_dist] [Rank 0] step=920, skipped=0, lr=[3.2000000000000005e-05], mom=[(0.9, 0.99)]\n",
      "[2024-10-30 13:11:49,297] [INFO] [timer.py:260:stop] epoch=0/micro_step=920/global_step=920, RunningAvgSamplesPerSec=0.8288634220977539, CurrSamplesPerSec=1.7912150451585476, MemAllocated=0.76GB, MaxMemAllocated=20.76GB\n",
      "100%|█| 92/92 [14:41<00:00,  9.58s/it, Epoch=10/10, loss=0.5043, dice_score=0.73\n",
      "100%|███████████████████████████████████████████| 12/12 [12:02<00:00, 60.18s/it]\n",
      "10 - 0.5043 - 0.7310 - 0.7527 - 0.0000 - 1.0000\n",
      "train_valid_test: 733_92_92\n",
      "train_valid_test: 733_92_92\n",
      "train_valid_test: 733_92_92\n",
      "model_param_num = 24001133\n",
      "model_param_num = 24001133\n",
      "model_param_num = 24001133\n",
      "train_valid_test: 733_92_92\n",
      "model_param_num = 24001133\n",
      "[2024-10-30 13:23:54,164] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.6, git-hash=unknown, git-branch=unknown\n",
      "[2024-10-30 13:23:54,283] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "Adam Optimizer #9 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "Adam Optimizer #9 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "Adam Optimizer #9 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "Adam Optimizer #9 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "[2024-10-30 13:23:55,999] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
      "[2024-10-30 13:23:55,999] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-10-30 13:23:56,029] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "[2024-10-30 13:23:56,030] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "[2024-10-30 13:23:56,030] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer\n",
      "[2024-10-30 13:23:56,030] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 200000000\n",
      "[2024-10-30 13:23:56,030] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 200000000\n",
      "[2024-10-30 13:23:56,030] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: True\n",
      "[2024-10-30 13:23:56,030] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-30 13:23:56,787] [INFO] [utils.py:791:see_memory_usage] Before initializing optimizer states\n",
      "[2024-10-30 13:23:56,788] [INFO] [utils.py:792:see_memory_usage] MA 0.69 GB         Max_MA 0.79 GB         CA 0.94 GB         Max_CA 1 GB \n",
      "[2024-10-30 13:23:56,788] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 85.25 GB, percent = 33.9%\n",
      "[2024-10-30 13:23:57,005] [INFO] [utils.py:791:see_memory_usage] After initializing optimizer states\n",
      "[2024-10-30 13:23:57,006] [INFO] [utils.py:792:see_memory_usage] MA 0.69 GB         Max_MA 0.69 GB         CA 0.94 GB         Max_CA 1 GB \n",
      "[2024-10-30 13:23:57,006] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 86.54 GB, percent = 34.4%\n",
      "[2024-10-30 13:23:57,006] [INFO] [stage_1_and_2.py:516:__init__] optimizer state initialized\n",
      "Adam Optimizer #10 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "[2024-10-30 13:23:57,165] [INFO] [utils.py:791:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-10-30 13:23:57,165] [INFO] [utils.py:792:see_memory_usage] MA 0.69 GB         Max_MA 0.69 GB         CA 0.94 GB         Max_CA 1 GB \n",
      "[2024-10-30 13:23:57,166] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 87.48 GB, percent = 34.8%\n",
      "[2024-10-30 13:23:57,175] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n",
      "[2024-10-30 13:23:57,175] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = OneCycle\n",
      "[2024-10-30 13:23:57,175] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.OneCycle object at 0x7fc07b729bd0>\n",
      "[2024-10-30 13:23:57,175] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[4e-05], mom=[(0.8, 0.99)]\n",
      "[2024-10-30 13:23:57,178] [INFO] [config.py:984:print] DeepSpeedEngine configuration:\n",
      "[2024-10-30 13:23:57,179] [INFO] [config.py:988:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-10-30 13:23:57,179] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-10-30 13:23:57,179] [INFO] [config.py:988:print]   amp_enabled .................. False\n",
      "[2024-10-30 13:23:57,179] [INFO] [config.py:988:print]   amp_params ................... False\n",
      "[2024-10-30 13:23:57,179] [INFO] [config.py:988:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-10-30 13:23:57,179] [INFO] [config.py:988:print]   bfloat16_enabled ............. False\n",
      "[2024-10-30 13:23:57,179] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-10-30 13:23:57,179] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-10-30 13:23:57,179] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-10-30 13:23:57,179] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc07b72a5f0>\n",
      "[2024-10-30 13:23:57,179] [INFO] [config.py:988:print]   communication_data_type ...... None\n",
      "[2024-10-30 13:23:57,179] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-10-30 13:23:57,179] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False\n",
      "[2024-10-30 13:23:57,179] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False\n",
      "[2024-10-30 13:23:57,179] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-10-30 13:23:57,179] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False\n",
      "[2024-10-30 13:23:57,179] [INFO] [config.py:988:print]   dataloader_drop_last ......... False\n",
      "[2024-10-30 13:23:57,179] [INFO] [config.py:988:print]   disable_allgather ............ False\n",
      "[2024-10-30 13:23:57,179] [INFO] [config.py:988:print]   dump_state ................... False\n",
      "[2024-10-30 13:23:57,179] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-10-30 13:23:57,179] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False\n",
      "[2024-10-30 13:23:57,179] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-10-30 13:23:57,179] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-10-30 13:23:57,179] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-10-30 13:23:57,179] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-10-30 13:23:57,179] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-10-30 13:23:57,179] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-10-30 13:23:57,179] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False\n",
      "[2024-10-30 13:23:57,179] [INFO] [config.py:988:print]   elasticity_enabled ........... False\n",
      "[2024-10-30 13:23:57,179] [INFO] [config.py:988:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   fp16_auto_cast ............... None\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   fp16_enabled ................. False\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   global_rank .................. 0\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   grad_accum_dtype ............. None\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   gradient_clipping ............ 1.0\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   graph_harvesting ............. False\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 65536\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   load_universal_checkpoint .... False\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   loss_scale ................... 0\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   memory_breakdown ............. False\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   mics_shard_size .............. -1\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   optimizer_name ............... adam\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   optimizer_params ............. {'lr': 1e-05, 'betas': [0.9, 0.999]}\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   pld_enabled .................. False\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   pld_params ................... False\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   prescale_gradients ........... False\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   scheduler_name ............... OneCycle\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   scheduler_params ............. {'cycle_min_lr': 4e-05, 'cycle_max_lr': 0.0002, 'decay_lr_rate': 0.25, 'cycle_first_step_size': 184, 'cycle_second_step_size': 460, 'decay_step_size': 276}\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   sparse_attention ............. None\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   steps_per_print .............. 92\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   train_batch_size ............. 8\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  2\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   use_node_local_storage ....... False\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   weight_quantization_config ... None\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   world_size ................... 4\n",
      "[2024-10-30 13:23:57,180] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False\n",
      "[2024-10-30 13:23:57,181] [INFO] [config.py:988:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=PosixPath('/local_nvme'), buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=PosixPath('/local_nvme'), buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-10-30 13:23:57,181] [INFO] [config.py:988:print]   zero_enabled ................. True\n",
      "[2024-10-30 13:23:57,181] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-10-30 13:23:57,181] [INFO] [config.py:988:print]   zero_optimization_stage ...... 2\n",
      "[2024-10-30 13:23:57,181] [INFO] [config.py:974:print_user_config]   json = {\n",
      "    \"train_batch_size\": 8, \n",
      "    \"train_micro_batch_size_per_gpu\": 2, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"pipeline_parallel\": true, \n",
      "    \"steps_per_print\": 92, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"hysteresis\": 2, \n",
      "        \"consecutive_hysteresis\": false, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"nvme_path\": \"/local_nvme\", \n",
      "            \"pin_memory\": true, \n",
      "            \"buffer_count\": 4, \n",
      "            \"fast_init\": false\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"nvme_path\": \"/local_nvme\", \n",
      "            \"pin_memory\": true, \n",
      "            \"buffer_count\": 5, \n",
      "            \"buffer_size\": 1.000000e+08, \n",
      "            \"max_in_cpu\": 1.000000e+09\n",
      "        }\n",
      "    }, \n",
      "    \"data_sampling\": {\n",
      "        \"enabled\": true, \n",
      "        \"num_workers\": 16\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"OneCycle\", \n",
      "        \"params\": {\n",
      "            \"cycle_min_lr\": 4e-05, \n",
      "            \"cycle_max_lr\": 0.0002, \n",
      "            \"decay_lr_rate\": 0.25, \n",
      "            \"cycle_first_step_size\": 184, \n",
      "            \"cycle_second_step_size\": 460, \n",
      "            \"decay_step_size\": 276\n",
      "        }\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"Adam\", \n",
      "        \"params\": {\n",
      "            \"lr\": 1e-05, \n",
      "            \"betas\": [0.9, 0.999]\n",
      "        }\n",
      "    }\n",
      "}\n",
      "[2024-10-30 13:23:57,181] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.6, git-hash=unknown, git-branch=unknown\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-30 13:23:57,225] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "Adam Optimizer #10 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "Adam Optimizer #10 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "Adam Optimizer #10 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "[2024-10-30 13:23:58,867] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
      "[2024-10-30 13:23:58,867] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-10-30 13:23:58,895] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "[2024-10-30 13:23:58,895] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "[2024-10-30 13:23:58,895] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer\n",
      "[2024-10-30 13:23:58,895] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 200000000\n",
      "[2024-10-30 13:23:58,895] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 200000000\n",
      "[2024-10-30 13:23:58,895] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: True\n",
      "[2024-10-30 13:23:58,895] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False\n",
      "Adam Optimizer #11 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "Adam Optimizer #11 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "Adam Optimizer #11 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "[2024-10-30 13:23:59,324] [INFO] [utils.py:791:see_memory_usage] Before initializing optimizer states\n",
      "[2024-10-30 13:23:59,326] [INFO] [utils.py:792:see_memory_usage] MA 0.79 GB         Max_MA 0.79 GB         CA 0.94 GB         Max_CA 1 GB \n",
      "[2024-10-30 13:23:59,327] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 92.14 GB, percent = 36.6%\n",
      "[2024-10-30 13:23:59,579] [INFO] [utils.py:791:see_memory_usage] After initializing optimizer states\n",
      "[2024-10-30 13:23:59,580] [INFO] [utils.py:792:see_memory_usage] MA 0.79 GB         Max_MA 0.79 GB         CA 0.94 GB         Max_CA 1 GB \n",
      "[2024-10-30 13:23:59,580] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 92.14 GB, percent = 36.6%\n",
      "[2024-10-30 13:23:59,580] [INFO] [stage_1_and_2.py:516:__init__] optimizer state initialized\n",
      "[2024-10-30 13:23:59,745] [INFO] [utils.py:791:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-10-30 13:23:59,746] [INFO] [utils.py:792:see_memory_usage] MA 0.79 GB         Max_MA 0.79 GB         CA 0.94 GB         Max_CA 1 GB \n",
      "[2024-10-30 13:23:59,746] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 92.14 GB, percent = 36.6%\n",
      "[2024-10-30 13:23:59,761] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n",
      "[2024-10-30 13:23:59,761] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = OneCycle\n",
      "[2024-10-30 13:23:59,761] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.OneCycle object at 0x7fc0363ad180>\n",
      "[2024-10-30 13:23:59,762] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[4e-05], mom=[(0.8, 0.99)]\n",
      "[2024-10-30 13:23:59,765] [INFO] [config.py:984:print] DeepSpeedEngine configuration:\n",
      "[2024-10-30 13:23:59,765] [INFO] [config.py:988:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-10-30 13:23:59,765] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-10-30 13:23:59,765] [INFO] [config.py:988:print]   amp_enabled .................. False\n",
      "[2024-10-30 13:23:59,765] [INFO] [config.py:988:print]   amp_params ................... False\n",
      "[2024-10-30 13:23:59,765] [INFO] [config.py:988:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-10-30 13:23:59,765] [INFO] [config.py:988:print]   bfloat16_enabled ............. False\n",
      "[2024-10-30 13:23:59,765] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-10-30 13:23:59,765] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-10-30 13:23:59,765] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-10-30 13:23:59,765] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc07b72a0b0>\n",
      "[2024-10-30 13:23:59,765] [INFO] [config.py:988:print]   communication_data_type ...... None\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   dataloader_drop_last ......... False\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   disable_allgather ............ False\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   dump_state ................... False\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   elasticity_enabled ........... False\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   fp16_auto_cast ............... None\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   fp16_enabled ................. False\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   global_rank .................. 0\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   grad_accum_dtype ............. None\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   gradient_clipping ............ 1.0\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   graph_harvesting ............. False\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 65536\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   load_universal_checkpoint .... False\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   loss_scale ................... 0\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   memory_breakdown ............. False\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   mics_shard_size .............. -1\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-10-30 13:23:59,766] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-10-30 13:23:59,767] [INFO] [config.py:988:print]   optimizer_name ............... adam\n",
      "[2024-10-30 13:23:59,767] [INFO] [config.py:988:print]   optimizer_params ............. {'lr': 1e-05, 'betas': [0.9, 0.999]}\n",
      "[2024-10-30 13:23:59,767] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-10-30 13:23:59,767] [INFO] [config.py:988:print]   pld_enabled .................. False\n",
      "[2024-10-30 13:23:59,767] [INFO] [config.py:988:print]   pld_params ................... False\n",
      "[2024-10-30 13:23:59,767] [INFO] [config.py:988:print]   prescale_gradients ........... False\n",
      "[2024-10-30 13:23:59,767] [INFO] [config.py:988:print]   scheduler_name ............... OneCycle\n",
      "[2024-10-30 13:23:59,767] [INFO] [config.py:988:print]   scheduler_params ............. {'cycle_min_lr': 4e-05, 'cycle_max_lr': 0.0002, 'decay_lr_rate': 0.25, 'cycle_first_step_size': 184, 'cycle_second_step_size': 460, 'decay_step_size': 276}\n",
      "[2024-10-30 13:23:59,767] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-10-30 13:23:59,767] [INFO] [config.py:988:print]   sparse_attention ............. None\n",
      "[2024-10-30 13:23:59,767] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False\n",
      "[2024-10-30 13:23:59,767] [INFO] [config.py:988:print]   steps_per_print .............. 92\n",
      "[2024-10-30 13:23:59,767] [INFO] [config.py:988:print]   train_batch_size ............. 8\n",
      "[2024-10-30 13:23:59,767] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  2\n",
      "[2024-10-30 13:23:59,767] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False\n",
      "[2024-10-30 13:23:59,767] [INFO] [config.py:988:print]   use_node_local_storage ....... False\n",
      "[2024-10-30 13:23:59,767] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False\n",
      "[2024-10-30 13:23:59,767] [INFO] [config.py:988:print]   weight_quantization_config ... None\n",
      "[2024-10-30 13:23:59,767] [INFO] [config.py:988:print]   world_size ................... 4\n",
      "[2024-10-30 13:23:59,767] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False\n",
      "[2024-10-30 13:23:59,767] [INFO] [config.py:988:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=PosixPath('/local_nvme'), buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=PosixPath('/local_nvme'), buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-10-30 13:23:59,767] [INFO] [config.py:988:print]   zero_enabled ................. True\n",
      "[2024-10-30 13:23:59,767] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-10-30 13:23:59,767] [INFO] [config.py:988:print]   zero_optimization_stage ...... 2\n",
      "[2024-10-30 13:23:59,767] [INFO] [config.py:974:print_user_config]   json = {\n",
      "    \"train_batch_size\": 8, \n",
      "    \"train_micro_batch_size_per_gpu\": 2, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"pipeline_parallel\": true, \n",
      "    \"steps_per_print\": 92, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"hysteresis\": 2, \n",
      "        \"consecutive_hysteresis\": false, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"nvme_path\": \"/local_nvme\", \n",
      "            \"pin_memory\": true, \n",
      "            \"buffer_count\": 4, \n",
      "            \"fast_init\": false\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"nvme_path\": \"/local_nvme\", \n",
      "            \"pin_memory\": true, \n",
      "            \"buffer_count\": 5, \n",
      "            \"buffer_size\": 1.000000e+08, \n",
      "            \"max_in_cpu\": 1.000000e+09\n",
      "        }\n",
      "    }, \n",
      "    \"data_sampling\": {\n",
      "        \"enabled\": true, \n",
      "        \"num_workers\": 16\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"OneCycle\", \n",
      "        \"params\": {\n",
      "            \"cycle_min_lr\": 4e-05, \n",
      "            \"cycle_max_lr\": 0.0002, \n",
      "            \"decay_lr_rate\": 0.25, \n",
      "            \"cycle_first_step_size\": 184, \n",
      "            \"cycle_second_step_size\": 460, \n",
      "            \"decay_step_size\": 276\n",
      "        }\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"Adam\", \n",
      "        \"params\": {\n",
      "            \"lr\": 1e-05, \n",
      "            \"betas\": [0.9, 0.999]\n",
      "        }\n",
      "    }\n",
      "}\n",
      "[2024-10-30 13:23:59,768] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.6, git-hash=unknown, git-branch=unknown\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-30 13:23:59,820] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "Adam Optimizer #11 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000010, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "[2024-10-30 13:24:01,562] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
      "[2024-10-30 13:24:01,562] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-10-30 13:24:01,589] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "[2024-10-30 13:24:01,589] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "[2024-10-30 13:24:01,589] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer\n",
      "[2024-10-30 13:24:01,589] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 200000000\n",
      "[2024-10-30 13:24:01,589] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 200000000\n",
      "[2024-10-30 13:24:01,589] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: True\n",
      "[2024-10-30 13:24:01,589] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False\n",
      "[2024-10-30 13:24:02,094] [INFO] [utils.py:791:see_memory_usage] Before initializing optimizer states\n",
      "[2024-10-30 13:24:02,095] [INFO] [utils.py:792:see_memory_usage] MA 0.79 GB         Max_MA 0.9 GB         CA 1.03 GB         Max_CA 1 GB \n",
      "[2024-10-30 13:24:02,095] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 93.36 GB, percent = 37.1%\n",
      "[2024-10-30 13:24:02,309] [INFO] [utils.py:791:see_memory_usage] After initializing optimizer states\n",
      "[2024-10-30 13:24:02,310] [INFO] [utils.py:792:see_memory_usage] MA 0.79 GB         Max_MA 0.79 GB         CA 1.03 GB         Max_CA 1 GB \n",
      "[2024-10-30 13:24:02,310] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 93.66 GB, percent = 37.2%\n",
      "[2024-10-30 13:24:02,310] [INFO] [stage_1_and_2.py:516:__init__] optimizer state initialized\n",
      "[2024-10-30 13:24:02,473] [INFO] [utils.py:791:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-10-30 13:24:02,473] [INFO] [utils.py:792:see_memory_usage] MA 0.79 GB         Max_MA 0.79 GB         CA 1.03 GB         Max_CA 1 GB \n",
      "[2024-10-30 13:24:02,474] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 93.79 GB, percent = 37.3%\n",
      "[2024-10-30 13:24:02,489] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n",
      "[2024-10-30 13:24:02,489] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = OneCycle\n",
      "[2024-10-30 13:24:02,489] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.OneCycle object at 0x7fc109107df0>\n",
      "[2024-10-30 13:24:02,489] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[4e-05], mom=[(0.8, 0.99)]\n",
      "[2024-10-30 13:24:02,492] [INFO] [config.py:984:print] DeepSpeedEngine configuration:\n",
      "[2024-10-30 13:24:02,492] [INFO] [config.py:988:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-10-30 13:24:02,492] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-10-30 13:24:02,492] [INFO] [config.py:988:print]   amp_enabled .................. False\n",
      "[2024-10-30 13:24:02,492] [INFO] [config.py:988:print]   amp_params ................... False\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   bfloat16_enabled ............. False\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc080219cf0>\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   communication_data_type ...... None\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   dataloader_drop_last ......... False\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   disable_allgather ............ False\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   dump_state ................... False\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   elasticity_enabled ........... False\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   fp16_auto_cast ............... None\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   fp16_enabled ................. False\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   global_rank .................. 0\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   grad_accum_dtype ............. None\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   gradient_clipping ............ 1.0\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-10-30 13:24:02,493] [INFO] [config.py:988:print]   graph_harvesting ............. False\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 65536\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   load_universal_checkpoint .... False\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   loss_scale ................... 0\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   memory_breakdown ............. False\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   mics_shard_size .............. -1\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   optimizer_name ............... adam\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   optimizer_params ............. {'lr': 1e-05, 'betas': [0.9, 0.999]}\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   pld_enabled .................. False\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   pld_params ................... False\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   prescale_gradients ........... False\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   scheduler_name ............... OneCycle\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   scheduler_params ............. {'cycle_min_lr': 4e-05, 'cycle_max_lr': 0.0002, 'decay_lr_rate': 0.25, 'cycle_first_step_size': 184, 'cycle_second_step_size': 460, 'decay_step_size': 276}\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   sparse_attention ............. None\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   steps_per_print .............. 92\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   train_batch_size ............. 8\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  2\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   use_node_local_storage ....... False\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   weight_quantization_config ... None\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   world_size ................... 4\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=PosixPath('/local_nvme'), buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=PosixPath('/local_nvme'), buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   zero_enabled ................. True\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-10-30 13:24:02,494] [INFO] [config.py:988:print]   zero_optimization_stage ...... 2\n",
      "[2024-10-30 13:24:02,495] [INFO] [config.py:974:print_user_config]   json = {\n",
      "    \"train_batch_size\": 8, \n",
      "    \"train_micro_batch_size_per_gpu\": 2, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"pipeline_parallel\": true, \n",
      "    \"steps_per_print\": 92, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"hysteresis\": 2, \n",
      "        \"consecutive_hysteresis\": false, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"nvme_path\": \"/local_nvme\", \n",
      "            \"pin_memory\": true, \n",
      "            \"buffer_count\": 4, \n",
      "            \"fast_init\": false\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"nvme_path\": \"/local_nvme\", \n",
      "            \"pin_memory\": true, \n",
      "            \"buffer_count\": 5, \n",
      "            \"buffer_size\": 1.000000e+08, \n",
      "            \"max_in_cpu\": 1.000000e+09\n",
      "        }\n",
      "    }, \n",
      "    \"data_sampling\": {\n",
      "        \"enabled\": true, \n",
      "        \"num_workers\": 16\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"OneCycle\", \n",
      "        \"params\": {\n",
      "            \"cycle_min_lr\": 4e-05, \n",
      "            \"cycle_max_lr\": 0.0002, \n",
      "            \"decay_lr_rate\": 0.25, \n",
      "            \"cycle_first_step_size\": 184, \n",
      "            \"cycle_second_step_size\": 460, \n",
      "            \"decay_step_size\": 276\n",
      "        }\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"Adam\", \n",
      "        \"params\": {\n",
      "            \"lr\": 1e-05, \n",
      "            \"betas\": [0.9, 0.999]\n",
      "        }\n",
      "    }\n",
      "}\n",
      "torch.float32\n",
      "./result/3d_unet_custom_propotional_bce_8_32_class_recon_fold_3/log.csv check exist...\n",
      "./result/3d_unet_custom_propotional_bce_8_32_class_recon_fold_3/log.csv does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99%|▉| 91/92 [15:26<00:07,  7.44s/it, Epoch=1/10, loss=0.8994, dice_score=0.384[2024-10-30 13:39:32,898] [INFO] [logging.py:96:log_dist] [Rank 0] step=92, skipped=0, lr=[0.00011999999999999996], mom=[(0.8500000000000001, 0.99)]\n",
      "[2024-10-30 13:39:32,903] [INFO] [timer.py:260:stop] epoch=0/micro_step=92/global_step=92, RunningAvgSamplesPerSec=0.8008329837178111, CurrSamplesPerSec=1.831547905518148, MemAllocated=0.86GB, MaxMemAllocated=20.87GB\n",
      "100%|█| 92/92 [15:30<00:00, 10.12s/it, Epoch=1/10, loss=0.8957, dice_score=0.388\n",
      "100%|███████████████████████████████████████████| 12/12 [10:21<00:00, 51.78s/it]\n",
      "1 - 0.8957 - 0.3888 - 0.7242 - 0.1641 - 1.0000\n",
      " 99%|▉| 91/92 [15:00<00:07,  7.29s/it, Epoch=2/10, loss=0.5646, dice_score=0.750[2024-10-30 14:05:01,479] [INFO] [logging.py:96:log_dist] [Rank 0] step=184, skipped=0, lr=[0.00019999999999999993], mom=[(0.8, 0.99)]\n",
      "[2024-10-30 14:05:01,527] [INFO] [timer.py:260:stop] epoch=0/micro_step=184/global_step=184, RunningAvgSamplesPerSec=0.8071945130180238, CurrSamplesPerSec=1.9068963073554024, MemAllocated=0.86GB, MaxMemAllocated=20.87GB\n",
      "100%|█| 92/92 [15:05<00:00,  9.84s/it, Epoch=2/10, loss=0.5658, dice_score=0.748\n",
      "100%|███████████████████████████████████████████| 12/12 [10:05<00:00, 50.43s/it]\n",
      "2 - 0.5658 - 0.7486 - 0.7486 - 0.0002 - 1.0000\n",
      " 99%|▉| 91/92 [14:55<00:06,  6.87s/it, Epoch=3/10, loss=0.5564, dice_score=0.744[2024-10-30 14:30:09,456] [INFO] [logging.py:96:log_dist] [Rank 0] step=276, skipped=0, lr=[0.000168], mom=[(0.8200000000000001, 0.99)]\n",
      "[2024-10-30 14:30:09,476] [INFO] [timer.py:260:stop] epoch=0/micro_step=276/global_step=276, RunningAvgSamplesPerSec=0.8108378317902395, CurrSamplesPerSec=1.8635681722604833, MemAllocated=0.86GB, MaxMemAllocated=20.87GB\n",
      "100%|█| 92/92 [15:00<00:00,  9.79s/it, Epoch=3/10, loss=0.5556, dice_score=0.744\n",
      "100%|███████████████████████████████████████████| 12/12 [10:23<00:00, 51.93s/it]\n",
      "3 - 0.5556 - 0.7446 - 0.7446 - 0.0001 - 1.0000\n",
      " 99%|▉| 91/92 [15:07<00:06,  6.98s/it, Epoch=4/10, loss=0.5204, dice_score=0.744[2024-10-30 14:55:46,284] [INFO] [logging.py:96:log_dist] [Rank 0] step=368, skipped=0, lr=[0.000136], mom=[(0.8400000000000001, 0.99)]\n",
      "[2024-10-30 14:55:46,287] [INFO] [timer.py:260:stop] epoch=0/micro_step=368/global_step=368, RunningAvgSamplesPerSec=0.8100519023900776, CurrSamplesPerSec=1.869698815425034, MemAllocated=0.86GB, MaxMemAllocated=20.87GB\n",
      "100%|█| 92/92 [15:12<00:00,  9.91s/it, Epoch=4/10, loss=0.5172, dice_score=0.747\n",
      "100%|███████████████████████████████████████████| 12/12 [10:00<00:00, 50.04s/it]\n",
      "4 - 0.5172 - 0.7473 - 0.7473 - 0.0000 - 1.0000\n",
      " 99%|▉| 91/92 [15:07<00:07,  7.13s/it, Epoch=5/10, loss=0.5331, dice_score=0.736[2024-10-30 15:21:01,645] [INFO] [logging.py:96:log_dist] [Rank 0] step=460, skipped=0, lr=[0.00010399999999999998], mom=[(0.8600000000000001, 0.99)]\n",
      "[2024-10-30 15:21:01,650] [INFO] [timer.py:260:stop] epoch=0/micro_step=460/global_step=460, RunningAvgSamplesPerSec=0.809611647181612, CurrSamplesPerSec=1.873916015755346, MemAllocated=0.86GB, MaxMemAllocated=20.87GB\n",
      "100%|█| 92/92 [15:12<00:00,  9.91s/it, Epoch=5/10, loss=0.5342, dice_score=0.735\n",
      "100%|███████████████████████████████████████████| 12/12 [09:56<00:00, 49.71s/it]\n",
      "5 - 0.5342 - 0.7351 - 0.7351 - 0.0000 - 1.0000\n",
      " 99%|▉| 91/92 [15:02<00:07,  7.19s/it, Epoch=6/10, loss=0.5002, dice_score=0.730[2024-10-30 15:46:08,120] [INFO] [logging.py:96:log_dist] [Rank 0] step=552, skipped=0, lr=[7.199999999999999e-05], mom=[(0.88, 0.99)]\n",
      "[2024-10-30 15:46:08,183] [INFO] [timer.py:260:stop] epoch=0/micro_step=552/global_step=552, RunningAvgSamplesPerSec=0.810032212620194, CurrSamplesPerSec=1.7836872488150546, MemAllocated=0.86GB, MaxMemAllocated=20.87GB\n",
      "100%|█| 92/92 [15:07<00:00,  9.86s/it, Epoch=6/10, loss=0.4998, dice_score=0.732\n",
      "100%|███████████████████████████████████████████| 12/12 [10:08<00:00, 50.68s/it]\n",
      "6 - 0.4998 - 0.7323 - 0.7432 - 0.0000 - 1.0000\n",
      " 99%|▉| 91/92 [15:01<00:07,  7.19s/it, Epoch=7/10, loss=0.5156, dice_score=0.736[2024-10-30 16:11:24,654] [INFO] [logging.py:96:log_dist] [Rank 0] step=644, skipped=0, lr=[4e-05], mom=[(0.9, 0.99)]\n",
      "[2024-10-30 16:11:24,689] [INFO] [timer.py:260:stop] epoch=0/micro_step=644/global_step=644, RunningAvgSamplesPerSec=0.810462045406426, CurrSamplesPerSec=1.8133855029385935, MemAllocated=0.86GB, MaxMemAllocated=20.87GB\n",
      "100%|█| 92/92 [15:06<00:00,  9.85s/it, Epoch=7/10, loss=0.5141, dice_score=0.736\n",
      "100%|███████████████████████████████████████████| 12/12 [10:30<00:00, 52.57s/it]\n",
      "7 - 0.5141 - 0.7364 - 0.7446 - 0.0000 - 1.0000\n",
      " 99%|▉| 91/92 [15:03<00:07,  7.85s/it, Epoch=8/10, loss=0.4710, dice_score=0.737[2024-10-30 16:37:07,023] [INFO] [logging.py:96:log_dist] [Rank 0] step=736, skipped=0, lr=[3.692307692307693e-05], mom=[(0.9, 0.99)]\n",
      "[2024-10-30 16:37:07,029] [INFO] [timer.py:260:stop] epoch=0/micro_step=736/global_step=736, RunningAvgSamplesPerSec=0.8105264615092629, CurrSamplesPerSec=1.7943434257657318, MemAllocated=0.86GB, MaxMemAllocated=20.87GB\n",
      "100%|█| 92/92 [15:08<00:00,  9.88s/it, Epoch=8/10, loss=0.4674, dice_score=0.740\n",
      "100%|███████████████████████████████████████████| 12/12 [10:30<00:00, 52.55s/it]\n",
      "8 - 0.4674 - 0.7405 - 0.7962 - 0.0000 - 1.0000\n",
      " 99%|▉| 91/92 [14:58<00:06,  6.85s/it, Epoch=9/10, loss=0.5050, dice_score=0.728[2024-10-30 17:02:42,236] [INFO] [logging.py:96:log_dist] [Rank 0] step=828, skipped=0, lr=[3.4285714285714284e-05], mom=[(0.9, 0.99)]\n",
      "[2024-10-30 17:02:42,239] [INFO] [timer.py:260:stop] epoch=0/micro_step=828/global_step=828, RunningAvgSamplesPerSec=0.81114266606775, CurrSamplesPerSec=2.0472708846672343, MemAllocated=0.86GB, MaxMemAllocated=20.87GB\n",
      "100%|█| 92/92 [15:03<00:00,  9.82s/it, Epoch=9/10, loss=0.5059, dice_score=0.728\n",
      " 17%|███████▎                                    | 2/12 [02:37<12:03, 72.31s/it]"
     ]
    }
   ],
   "source": [
    "!deepspeed --include localhost:0,1,2,3 --master_port=29508 1_train_heterogeneity_deepspeed.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a10308f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct 29 17:52:47 2024       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA TITAN RTX               Off | 00000000:18:00.0 Off |                  N/A |\r\n",
      "| 41%   47C    P8              14W / 280W |      1MiB / 24576MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "|   1  NVIDIA TITAN RTX               Off | 00000000:3B:00.0 Off |                  N/A |\r\n",
      "| 41%   44C    P8              17W / 280W |      1MiB / 24576MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "|   2  NVIDIA TITAN RTX               Off | 00000000:5E:00.0 Off |                  N/A |\r\n",
      "| 41%   35C    P8               3W / 280W |      1MiB / 24576MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "|   3  NVIDIA TITAN RTX               Off | 00000000:86:00.0 Off |                  N/A |\r\n",
      "| 41%   39C    P8              13W / 280W |      1MiB / 24576MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|  No running processes found                                                           |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ebfd0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
